{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Bayesian Ranking - Core System\n",
    "\n",
    "Build and test the Bayesian ranking system on a single race.\n",
    "\n",
    "## What's In This Notebook\n",
    "\n",
    "1. **Prior generation** - Convert championship standings to Bayesian priors\n",
    "2. **Bayesian ranking system** - Update priors with evidence\n",
    "3. **Performance scoring** - Extract driver rankings from telemetry\n",
    "4. **Single race test** - Validate on Bahrain 2024\n",
    "5. **Temporal validation** - Prove we're not cheating with future data\n",
    "\n",
    "**Expected result:** MAE ~2.6-3.0 on single race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab284628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BAYESIAN VALIDATION: 2025 Season\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastf1\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "cache_dir = Path('../data/raw/.fastf1_cache')\n",
    "fastf1.Cache.enable_cache(str(cache_dir))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BAYESIAN VALIDATION: 2025 Season\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c7bb1",
   "metadata": {},
   "source": [
    "## PART 1: Tier-Based Priors (Realistic for 2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f44a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 1: Define Team/Driver Tiers\n",
      "================================================================================\n",
      "\n",
      "Initialized priors for 20 drivers:\n",
      "\n",
      "Team Tiers:\n",
      "  midfield     8 drivers\n",
      "  backmarker   8 drivers\n",
      "  top          4 drivers\n",
      "\n",
      "Sample priors:\n",
      "  VER (Red Bull Racing): Î¼=18.0, Ïƒ=4.0 [top]\n",
      "  NOR (McLaren     ): Î¼=18.0, Ïƒ=4.0 [top]\n",
      "  LEC (Ferrari     ): Î¼=14.0, Ïƒ=5.0 [midfield]\n",
      "  HUL (Kick Sauber ): Î¼=10.0, Ïƒ=6.0 [midfield]\n",
      "\n",
      "ðŸŸ¢ Priors defined\n",
      "\n",
      "Key properties:\n",
      "  - High uncertainty (Ïƒ=4-7) reflects 2026 regulation change\n",
      "  - Top teams: Î¼=15-18 (expected P3-P5 range)\n",
      "  - Backmarkers: Î¼=3-6 (expected P15-P18 range)\n",
      "  - NO exact positions - just tier expectations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: Define Team/Driver Tiers\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For 2026, we'd know organizational capability, not exact performance\n",
    "# Let's simulate this with 2025 data\n",
    "\n",
    "@dataclass\n",
    "class DriverPrior:\n",
    "    \"\"\"Prior belief about driver performance.\"\"\"\n",
    "    driver_number: str\n",
    "    driver_code: str\n",
    "    team: str\n",
    "    team_tier: str  # 'top', 'midfield', 'backmarker'\n",
    "    driver_tier: str  # 'elite', 'experienced', 'rookie'\n",
    "    \n",
    "    # Initial rating (Î¼, Ïƒ)\n",
    "    # Î¼ = expected position (higher = better, 20 = best, 1 = worst)\n",
    "    # Ïƒ = uncertainty (higher = more uncertain)\n",
    "    mu: float\n",
    "    sigma: float\n",
    "\n",
    "def initialize_2026_style_priors() -> Dict[str, DriverPrior]:\n",
    "    \"\"\"\n",
    "    Initialize priors based on 2025 championship standings.\n",
    "    \n",
    "    2025 Results inform tier structure:\n",
    "    - Top 4: NOR(423), VER(421), PIA(410), RUS(319) - Clear elite\n",
    "    - Next tier: LEC(242), HAM(156), ANT(150) - Strong but gap to top\n",
    "    - Midfield: ALB(73), SAI(64), ALO(56), HUL(51), HAD(51)\n",
    "    - Lower: BEA(41), LAW(38), OCO(38), STR(33), TSU(33)\n",
    "    - Backmarkers: GAS(22), BOR(19), COL(0)\n",
    "    \n",
    "    For 2026: Keep tier structure but HIGH uncertainty (new regs)\n",
    "    \"\"\"\n",
    "    priors = {\n",
    "        # TOP TIER - 2025 Top 4 (clear elite)\n",
    "        '4': DriverPrior('4', 'NOR', 'McLaren', 'top', 'elite', mu=18, sigma=4),  # Champion\n",
    "        '1': DriverPrior('1', 'VER', 'Red Bull Racing', 'top', 'elite', mu=18, sigma=4),  # 2 pts behind!\n",
    "        '81': DriverPrior('81', 'PIA', 'McLaren', 'top', 'elite', mu=17, sigma=4),  # Strong 3rd\n",
    "        '63': DriverPrior('63', 'RUS', 'Mercedes', 'top', 'elite', mu=17, sigma=4),  # Clear 4th\n",
    "        \n",
    "        # UPPER MIDFIELD - Next tier (strong but gap to top)\n",
    "        '16': DriverPrior('16', 'LEC', 'Ferrari', 'midfield', 'elite', mu=14, sigma=5),  # 242 pts\n",
    "        '44': DriverPrior('44', 'HAM', 'Ferrari', 'midfield', 'elite', mu=13, sigma=5),  # 156 pts (struggled)\n",
    "        '12': DriverPrior('12', 'ANT', 'Mercedes', 'midfield', 'rookie', mu=13, sigma=5),  # 150 pts (strong rookie)\n",
    "        \n",
    "        # MIDFIELD - Best of rest\n",
    "        '23': DriverPrior('23', 'ALB', 'Williams', 'midfield', 'experienced', mu=11, sigma=5),  # 73 pts\n",
    "        '55': DriverPrior('55', 'SAI', 'Williams', 'midfield', 'experienced', mu=11, sigma=5),  # 64 pts\n",
    "        '14': DriverPrior('14', 'ALO', 'Aston Martin', 'midfield', 'elite', mu=11, sigma=5),  # 56 pts\n",
    "        '27': DriverPrior('27', 'HUL', 'Kick Sauber', 'midfield', 'experienced', mu=10, sigma=6),  # 51 pts\n",
    "        '6': DriverPrior('6', 'HAD', 'Racing Bulls', 'midfield', 'rookie', mu=10, sigma=6),  # 51 pts (good rookie)\n",
    "        \n",
    "        # LOWER MIDFIELD\n",
    "        '87': DriverPrior('87', 'BEA', 'Haas F1 Team', 'backmarker', 'rookie', mu=9, sigma=6),  # 41 pts\n",
    "        '30': DriverPrior('30', 'LAW', 'Racing Bulls', 'backmarker', 'experienced', mu=9, sigma=6),  # 38 pts\n",
    "        '31': DriverPrior('31', 'OCO', 'Haas F1 Team', 'backmarker', 'experienced', mu=9, sigma=6),  # 38 pts\n",
    "        '18': DriverPrior('18', 'STR', 'Aston Martin', 'backmarker', 'experienced', mu=8, sigma=6),  # 33 pts\n",
    "        '22': DriverPrior('22', 'TSU', 'Red Bull Racing', 'backmarker', 'experienced', mu=8, sigma=6),  # 33 pts (struggled)\n",
    "        \n",
    "        # BACKMARKERS\n",
    "        '10': DriverPrior('10', 'GAS', 'Alpine', 'backmarker', 'experienced', mu=7, sigma=6),  # 22 pts\n",
    "        '5': DriverPrior('5', 'BOR', 'Kick Sauber', 'backmarker', 'rookie', mu=6, sigma=7),  # 19 pts\n",
    "        '7': DriverPrior('7', 'COL', 'Alpine', 'backmarker', 'rookie', mu=5, sigma=7),  # 0 pts (replaced DOO)\n",
    "    }\n",
    "    \n",
    "    return priors\n",
    "\n",
    "# Initialize priors\n",
    "priors = initialize_2026_style_priors()\n",
    "\n",
    "print(\"\\nInitialized priors for 20 drivers:\")\n",
    "print(f\"\\nTeam Tiers:\")\n",
    "tier_counts = {}\n",
    "for p in priors.values():\n",
    "    tier_counts[p.team_tier] = tier_counts.get(p.team_tier, 0) + 1\n",
    "\n",
    "for tier, count in sorted(tier_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {tier:12} {count} drivers\")\n",
    "\n",
    "print(f\"\\nSample priors:\")\n",
    "for driver_num in ['1', '4', '16', '27']:\n",
    "    p = priors[driver_num]\n",
    "    print(f\"  {p.driver_code:3} ({p.team:12}): Î¼={p.mu:4.1f}, Ïƒ={p.sigma:3.1f} [{p.team_tier}]\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ Priors defined\")\n",
    "print(\"\\nKey properties:\")\n",
    "print(\"  - High uncertainty (Ïƒ=4-7) reflects 2026 regulation change\")\n",
    "print(\"  - Top teams: Î¼=15-18 (expected P3-P5 range)\")\n",
    "print(\"  - Backmarkers: Î¼=3-6 (expected P15-P18 range)\")\n",
    "print(\"  - NO exact positions - just tier expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc95586",
   "metadata": {},
   "source": [
    "## PART 2: Bayesian Update Mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0aa40e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: Bayesian Ranking System\n",
      "================================================================================\n",
      "\n",
      "Initializing Bayesian ranking system...\n",
      "ðŸŸ¢ Model initialized with 20 drivers\n",
      "\n",
      "================================================================================\n",
      "INITIAL PREDICTIONS (Before Any Data)\n",
      "================================================================================\n",
      "\n",
      "Top 10 predicted:\n",
      " predicted_rank driver_code            team  predicted_position  ci_lower  ci_upper  rating_sigma\n",
      "              1         NOR         McLaren                   3       1.0     10.84             4\n",
      "              2         VER Red Bull Racing                   3       1.0     10.84             4\n",
      "              3         PIA         McLaren                   4       1.0     11.84             4\n",
      "              4         RUS        Mercedes                   4       1.0     11.84             4\n",
      "              5         LEC         Ferrari                   7       1.0     16.80             5\n",
      "              6         HAM         Ferrari                   8       1.0     17.80             5\n",
      "              7         ANT        Mercedes                   8       1.0     17.80             5\n",
      "              8         SAI        Williams                  10       1.0     19.80             5\n",
      "              9         ALO    Aston Martin                  10       1.0     19.80             5\n",
      "             10         ALB        Williams                  10       1.0     19.80             5\n",
      "\n",
      "Bottom 5 predicted:\n",
      " predicted_rank driver_code            team  predicted_position  ci_lower  ci_upper  rating_sigma\n",
      "             16         STR    Aston Martin                  13      1.24      20.0             6\n",
      "             17         TSU Red Bull Racing                  13      1.24      20.0             6\n",
      "             18         GAS          Alpine                  14      2.24      20.0             6\n",
      "             19         BOR     Kick Sauber                  15      1.28      20.0             7\n",
      "             20         COL          Alpine                  16      2.28      20.0             7\n",
      "\n",
      "================================================================================\n",
      "Key Observations:\n",
      "================================================================================\n",
      "  - Predictions based purely on team/driver tiers\n",
      "  - Average CI width: 16.7 positions\n",
      "  - Max uncertainty: 7.0\n",
      "  - This reflects: 'I know tiers but not exact order'\n",
      "\n",
      "ðŸŸ¢ Bayesian update mechanism ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: Bayesian Ranking System\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class BayesianDriverRanking:\n",
    "    \"\"\"\n",
    "    Bayesian ranking system with Gaussian priors.\n",
    "    \n",
    "    Each driver has rating ~ N(Î¼, ÏƒÂ²)\n",
    "    - Î¼ = performance rating (higher = better)\n",
    "    - Ïƒ = uncertainty (decreases as I observe more)\n",
    "    \n",
    "    Update rule: Bayesian inference with observed positions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, priors: Dict[str, DriverPrior]):\n",
    "        \"\"\"\n",
    "        Initialize with prior beliefs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        priors : dict\n",
    "            driver_number -> DriverPrior\n",
    "        \"\"\"\n",
    "        self.ratings = {}  # driver_number -> (mu, sigma)\n",
    "        self.n_observations = {}  # driver_number -> count\n",
    "        \n",
    "        for driver_num, prior in priors.items():\n",
    "            self.ratings[driver_num] = (prior.mu, prior.sigma)\n",
    "            self.n_observations[driver_num] = 0\n",
    "        \n",
    "        self.priors = priors\n",
    "        self.update_history = []  # Track all updates\n",
    "    \n",
    "    def predict_positions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict positions based on current ratings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns: driver_number, predicted_position, ci_lower, ci_upper\n",
    "            Sorted by predicted position (best first)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for driver_num, (mu, sigma) in self.ratings.items():\n",
    "            # Convert rating to position (invert scale)\n",
    "            # Rating 18 â†’ Position ~2-3\n",
    "            # Rating 10 â†’ Position ~10-11\n",
    "            # Rating 3  â†’ Position ~18-19\n",
    "            \n",
    "            # Simple linear mapping: position = 21 - rating\n",
    "            predicted_pos = 21 - mu\n",
    "            \n",
    "            # Confidence interval (95% = Â±1.96Ïƒ)\n",
    "            ci_lower = max(1, predicted_pos - 1.96 * sigma)\n",
    "            ci_upper = min(20, predicted_pos + 1.96 * sigma)\n",
    "            \n",
    "            predictions.append({\n",
    "                'driver_number': driver_num,\n",
    "                'driver_code': self.priors[driver_num].driver_code,\n",
    "                'team': self.priors[driver_num].team,\n",
    "                'rating_mu': mu,\n",
    "                'rating_sigma': sigma,\n",
    "                'predicted_position': predicted_pos,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'n_observations': self.n_observations[driver_num]\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(predictions)\n",
    "        df = df.sort_values('predicted_position')\n",
    "        df['predicted_rank'] = range(1, len(df) + 1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def update_from_session(\n",
    "        self,\n",
    "        observed_positions: Dict[str, int],\n",
    "        confidence_weight: float = 1.0,\n",
    "        session_name: str = \"Session\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update ratings based on observed positions.\n",
    "        \n",
    "        Uses Bayesian update with Gaussian likelihood.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        observed_positions : dict\n",
    "            driver_number -> finishing position (1-20)\n",
    "        confidence_weight : float\n",
    "            How much to trust this observation\n",
    "            - 0.1 = testing (low trust)\n",
    "            - 0.3 = practice (medium trust)\n",
    "            - 0.7 = qualifying (high trust)\n",
    "            - 1.0 = race (full trust)\n",
    "        session_name : str\n",
    "            For logging\n",
    "        \"\"\"\n",
    "        # Bayesian update: posterior = likelihood Ã— prior\n",
    "        # For Gaussian: new_sigmaÂ² = 1 / (1/prior_sigmaÂ² + 1/obs_sigmaÂ²)\n",
    "        #               new_mu = (prior_mu/prior_sigmaÂ² + obs_mu/obs_sigmaÂ²) Ã— new_sigmaÂ²\n",
    "        \n",
    "        for driver_num, observed_pos in observed_positions.items():\n",
    "            if driver_num not in self.ratings:\n",
    "                continue  # Skip unknown drivers\n",
    "            \n",
    "            prior_mu, prior_sigma = self.ratings[driver_num]\n",
    "            \n",
    "            # Convert observed position to rating\n",
    "            # Position 1 â†’ Rating ~19-20\n",
    "            # Position 10 â†’ Rating ~10-11\n",
    "            # Position 20 â†’ Rating ~1-2\n",
    "            observed_rating = 21 - observed_pos\n",
    "            \n",
    "            # Observation uncertainty (inversely proportional to confidence)\n",
    "            # Higher confidence = lower observation uncertainty\n",
    "            obs_sigma = 5.0 / confidence_weight  # Range: 5 (race) to 50 (testing)\n",
    "            \n",
    "            # Bayesian update\n",
    "            new_sigma_sq = 1 / (1/prior_sigma**2 + 1/obs_sigma**2)\n",
    "            new_sigma = np.sqrt(new_sigma_sq)\n",
    "            \n",
    "            new_mu = (prior_mu/prior_sigma**2 + observed_rating/obs_sigma**2) * new_sigma_sq\n",
    "            \n",
    "            # Update ratings\n",
    "            self.ratings[driver_num] = (new_mu, new_sigma)\n",
    "            self.n_observations[driver_num] += 1\n",
    "            \n",
    "            # Log update\n",
    "            self.update_history.append({\n",
    "                'session': session_name,\n",
    "                'driver_number': driver_num,\n",
    "                'driver_code': self.priors[driver_num].driver_code,\n",
    "                'observed_position': observed_pos,\n",
    "                'prior_mu': prior_mu,\n",
    "                'prior_sigma': prior_sigma,\n",
    "                'new_mu': new_mu,\n",
    "                'new_sigma': new_sigma,\n",
    "                'confidence_weight': confidence_weight\n",
    "            })\n",
    "    \n",
    "    def get_update_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary of all updates.\"\"\"\n",
    "        return pd.DataFrame(self.update_history)\n",
    "\n",
    "\n",
    "# TEST: Initialize and Make Initial Prediction\n",
    "\n",
    "print(\"\\nInitializing Bayesian ranking system...\")\n",
    "model = BayesianDriverRanking(priors)\n",
    "\n",
    "print(\"ðŸŸ¢ Model initialized with 20 drivers\")\n",
    "\n",
    "# Make initial prediction (before any data)\n",
    "initial_predictions = model.predict_positions()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIAL PREDICTIONS (Before Any Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTop 10 predicted:\")\n",
    "print(initial_predictions.head(10)[\n",
    "    ['predicted_rank', 'driver_code', 'team', 'predicted_position', 'ci_lower', 'ci_upper', 'rating_sigma']\n",
    "].to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 5 predicted:\")\n",
    "print(initial_predictions.tail(5)[\n",
    "    ['predicted_rank', 'driver_code', 'team', 'predicted_position', 'ci_lower', 'ci_upper', 'rating_sigma']\n",
    "].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Observations:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  - Predictions based purely on team/driver tiers\")\n",
    "print(f\"  - Average CI width: {(initial_predictions['ci_upper'] - initial_predictions['ci_lower']).mean():.1f} positions\")\n",
    "print(f\"  - Max uncertainty: {initial_predictions['rating_sigma'].max():.1f}\")\n",
    "print(f\"  - This reflects: 'I know tiers but not exact order'\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ Bayesian update mechanism ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4b8e4",
   "metadata": {},
   "source": [
    "## PART 3: Car Performance Profiles from Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557a96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Car Performance Profiles from Testing\n",
    "\n",
    "class CarPerformanceProfile:\n",
    "    \"\"\"\n",
    "    Extract car characteristics from telemetry features.\n",
    "    \n",
    "    Profile includes:\n",
    "    - Slow corner performance (0-100 km/h)\n",
    "    - Medium corner performance (100-200 km/h)\n",
    "    - High corner performance (200+ km/h)\n",
    "    - Straight-line speed (full throttle)\n",
    "    - Tire degradation (lap time delta over stint)\n",
    "    - Stability (throttle/brake smoothness)\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_from_testing(self, testing_features: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Convert testing features into car profile.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            team -> {\n",
    "                'slow_corner_advantage': float (km/h vs median),\n",
    "                'medium_corner_advantage': float,\n",
    "                'high_corner_advantage': float,\n",
    "                'straight_advantage': float,\n",
    "                'deg_index': float (lower = better),\n",
    "                'stability_index': float (higher = better)\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass  # We'll implement this\n",
    "\n",
    "class TrackCharacteristics:\n",
    "    \"\"\"\n",
    "    Database of track characteristics.\n",
    "    \n",
    "    For each circuit:\n",
    "    - % of lap in slow corners\n",
    "    - % of lap in medium corners  \n",
    "    - % of lap in high corners\n",
    "    - % of lap on straights\n",
    "    - Avg lap time (for normalization)\n",
    "    - Tire stress level\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_track_database(self) -> pd.DataFrame:\n",
    "        \"\"\"Load track characteristics for all circuits.\"\"\"\n",
    "        pass  # We'll build this\n",
    "\n",
    "class TrackSpecificPredictor:\n",
    "    \"\"\"\n",
    "    Predict positions by matching car profiles to track demands.\n",
    "    \n",
    "    Logic:\n",
    "    If McLaren has +8 km/h in medium corners\n",
    "    And Silverstone is 60% medium corners\n",
    "    Then McLaren gets performance boost at Silverstone\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict_for_track(\n",
    "        self,\n",
    "        car_profiles: dict,\n",
    "        track_chars: dict,\n",
    "        bayesian_priors: dict\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict race results for specific track.\n",
    "        \n",
    "        Combines:\n",
    "        1. Bayesian priors (team tier)\n",
    "        2. Car performance profile (strengths/weaknesses)\n",
    "        3. Track characteristics (demands)\n",
    "        \n",
    "        Returns predictions with uncertainty\n",
    "        \"\"\"\n",
    "        pass  # We'll implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1258c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Loaded 60 driver-sessions\n",
      "  Columns: 76\n",
      "\n",
      "Available corner speed columns:\n",
      "  ['slow_corner_speed', 'medium_corner_speed', 'high_corner_speed', 'slow_corner_speed_std', 'medium_corner_speed_std', 'high_corner_speed_std', 'slow_corner_speed_rel', 'medium_corner_speed_rel', 'high_corner_speed_rel', 'slow_corner_speed_std_rel', 'medium_corner_speed_std_rel', 'high_corner_speed_std_rel', 'slow_corner_speed_pct', 'medium_corner_speed_pct', 'high_corner_speed_pct', 'slow_corner_speed_std_pct', 'medium_corner_speed_std_pct', 'high_corner_speed_std_pct']\n"
     ]
    }
   ],
   "source": [
    "# Load 2024 testing features (from notebook 02)\n",
    "testing_2024 = pd.read_parquet('../data/processed/testing_files/2024_testing_features.parquet')\n",
    "\n",
    "print(f\"ðŸŸ¢ Loaded {len(testing_2024)} driver-sessions\")\n",
    "print(f\"  Columns: {len(testing_2024.columns)}\")\n",
    "print(f\"\\nAvailable corner speed columns:\")\n",
    "corner_cols = [c for c in testing_2024.columns if 'corner' in c.lower()]\n",
    "print(f\"  {corner_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805782f",
   "metadata": {},
   "source": [
    "### PART 3A: Extract Track Characteristics from 2024 Season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf4daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3A: Build Track Characteristics Database\n",
      "================================================================================\n",
      "\n",
      "Testing scoring methods on 2024 testing data...\n",
      "Practice 3 drivers: 0\n",
      "âš ï¸  No Practice 3 data, using Practice 1 instead\n",
      "Practice 1 drivers: 20\n",
      "\n",
      "================================================================================\n",
      "SCORING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Absolute Difference:\n",
      "  VER medium corners: 0.77\n",
      "  VER straights: -4.40\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 - Score: 2.60\n",
      "    Driver 22 - Score: 2.11\n",
      "    Driver 30 - Score: 1.95\n",
      "\n",
      "Ranking:\n",
      "  VER medium corners: 8.00\n",
      "  VER straights: 18.00\n",
      "  Top 3 medium corners (by rank):\n",
      "    Driver 77 - Score: 1.00\n",
      "    Driver 22 - Score: 2.00\n",
      "    Driver 30 - Score: 3.00\n",
      "\n",
      "Quantile:\n",
      "  VER medium corners: 2.00\n",
      "  VER straights: 1.00\n",
      "  Top 3 medium corners:\n",
      "    Driver 22 - Score: 3.00\n",
      "    Driver 24 - Score: 3.00\n",
      "    Driver 30 - Score: 3.00\n",
      "\n",
      "Z-Score:\n",
      "  VER medium corners: 0.35\n",
      "  VER straights: -0.77\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 - Score: 2.10\n",
      "    Driver 22 - Score: 1.63\n",
      "    Driver 30 - Score: 1.48\n",
      "\n",
      "================================================================================\n",
      "ðŸŸ¢ All scoring methods tested on 2024 testing data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3A: Build Track Characteristics Database\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PerformanceScoringMethod:\n",
    "    \"\"\"Base class for different scoring approaches.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Score drivers on each characteristic.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        testing_features : pd.DataFrame\n",
    "            Features extracted from testing\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns: driver_number, slow_corner_score, medium_corner_score, etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AbsoluteDifferenceScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = (value - median) in actual units.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed', \n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            \n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    median_val = testing_features[feature_col].median()\n",
    "                    score[f'{metric_name}_score'] = row[feature_col] - median_val\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class RankingScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = rank (1 = best, 20 = worst).\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            \n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns:\n",
    "                    # Rank (ascending=False: highest value = rank 1)\n",
    "                    rank = testing_features[feature_col].rank(ascending=False, method='min', na_option='keep')\n",
    "                    score[f'{metric_name}_score'] = rank.loc[idx]\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class QuantileScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = quantile tier (3 = top 25%, 2 = middle 50%, 1 = bottom 25%).\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            \n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    val = row[feature_col]\n",
    "                    q75 = testing_features[feature_col].quantile(0.75)\n",
    "                    q25 = testing_features[feature_col].quantile(0.25)\n",
    "                    \n",
    "                    if val >= q75:\n",
    "                        tier = 3  # Top 25%\n",
    "                    elif val >= q25:\n",
    "                        tier = 2  # Middle 50%\n",
    "                    else:\n",
    "                        tier = 1  # Bottom 25%\n",
    "                    \n",
    "                    score[f'{metric_name}_score'] = tier\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class ZScoreScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = standardized z-score.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            \n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    mean_val = testing_features[feature_col].mean()\n",
    "                    std_val = testing_features[feature_col].std()\n",
    "                    \n",
    "                    if std_val > 0:\n",
    "                        z_score = (row[feature_col] - mean_val) / std_val\n",
    "                        score[f'{metric_name}_score'] = z_score\n",
    "                    else:\n",
    "                        score[f'{metric_name}_score'] = 0\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "# TEST: Compare Scoring Methods\n",
    "\n",
    "print(\"\\nTesting scoring methods on 2024 testing data...\")\n",
    "\n",
    "# Filter to Practice 3 (most representative day)\n",
    "day3_features = testing_2024[testing_2024['session_type'] == 'Practice 3']\n",
    "\n",
    "print(f\"Practice 3 drivers: {len(day3_features)}\")\n",
    "\n",
    "# Check if I have any Practice 3 data\n",
    "if len(day3_features) == 0:\n",
    "    print(\"âš ï¸  No Practice 3 data, using Practice 1 instead\")\n",
    "    day3_features = testing_2024[testing_2024['session_type'] == 'Practice 1']\n",
    "    print(f\"Practice 1 drivers: {len(day3_features)}\")\n",
    "\n",
    "# Initialize scoring methods\n",
    "methods = {\n",
    "    'Absolute Difference': AbsoluteDifferenceScoring(),\n",
    "    'Ranking': RankingScoring(),\n",
    "    'Quantile': QuantileScoring(),\n",
    "    'Z-Score': ZScoreScoring()\n",
    "}\n",
    "\n",
    "scores_by_method = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCORING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    scores = method.score_drivers(day3_features)\n",
    "    scores_by_method[method_name] = scores\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    \n",
    "    # Show VER's scores\n",
    "    ver_row = scores[scores['driver_number'] == '1']\n",
    "    if len(ver_row) > 0:\n",
    "        print(f\"  VER medium corners: {ver_row['medium_corner_score'].values[0]:.2f}\")\n",
    "        print(f\"  VER straights: {ver_row['straight_score'].values[0]:.2f}\")\n",
    "    \n",
    "    # Show top 3 in medium corners\n",
    "    scores_clean = scores.dropna(subset=['medium_corner_score'])\n",
    "    \n",
    "    if method_name == 'Ranking':\n",
    "        top3 = scores_clean.nsmallest(3, 'medium_corner_score')\n",
    "        print(f\"  Top 3 medium corners (by rank):\")\n",
    "    else:\n",
    "        top3 = scores_clean.nlargest(3, 'medium_corner_score')\n",
    "        print(f\"  Top 3 medium corners:\")\n",
    "    \n",
    "    for idx, row in top3.iterrows():\n",
    "        print(f\"    Driver {row['driver_number']:2} - Score: {row['medium_corner_score']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŸ¢ All scoring methods tested on 2024 testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bef3b1",
   "metadata": {},
   "source": [
    "### PART 3B: Scoring Method Comparison Framework (COMPLETE REPLACEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5037af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3B: Scoring Method Comparison Framework\n",
      "================================================================================\n",
      "\n",
      "Loading 2024 testing features...\n",
      "ðŸŸ¢ Loaded 60 driver-sessions\n",
      "\n",
      "Scoring 20 drivers...\n",
      "\n",
      "Absolute Difference:\n",
      "  VER medium corners: 0.77\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 - 2.60\n",
      "    Driver 22 - 2.11\n",
      "    Driver 30 - 1.95\n",
      "\n",
      "Ranking:\n",
      "  VER medium corners: 8.00\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 - 1.00\n",
      "    Driver 22 - 2.00\n",
      "    Driver 30 - 3.00\n",
      "\n",
      "Quantile:\n",
      "  VER medium corners: 2.00\n",
      "  Top 3 medium corners:\n",
      "    Driver 22 - 3.00\n",
      "    Driver 24 - 3.00\n",
      "    Driver 30 - 3.00\n",
      "\n",
      "Z-Score:\n",
      "  VER medium corners: 0.35\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 - 2.10\n",
      "    Driver 22 - 1.63\n",
      "    Driver 30 - 1.48\n",
      "\n",
      "ðŸŸ¢ All scoring methods tested\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3B: Scoring Method Comparison Framework\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class PerformanceScoringMethod:\n",
    "    \"\"\"Base class for different scoring approaches.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Score drivers on each characteristic.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AbsoluteDifferenceScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = (value - median) in actual units.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed', \n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    median_val = testing_features[feature_col].median()\n",
    "                    score[f'{metric_name}_score'] = row[feature_col] - median_val\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class RankingScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = rank (1 = best, 20 = worst).\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns:\n",
    "                    rank = testing_features[feature_col].rank(ascending=False, method='min', na_option='keep')\n",
    "                    score[f'{metric_name}_score'] = rank.loc[idx]\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class QuantileScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = quantile tier (3 = top 25%, 2 = middle 50%, 1 = bottom 25%).\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    val = row[feature_col]\n",
    "                    q75 = testing_features[feature_col].quantile(0.75)\n",
    "                    q25 = testing_features[feature_col].quantile(0.25)\n",
    "                    \n",
    "                    if val >= q75:\n",
    "                        tier = 3\n",
    "                    elif val >= q25:\n",
    "                        tier = 2\n",
    "                    else:\n",
    "                        tier = 1\n",
    "                    \n",
    "                    score[f'{metric_name}_score'] = tier\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "class ZScoreScoring(PerformanceScoringMethod):\n",
    "    \"\"\"Score = standardized z-score.\"\"\"\n",
    "    \n",
    "    def score_drivers(self, testing_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = []\n",
    "        \n",
    "        features = {\n",
    "            'slow_corner': 'slow_corner_speed',\n",
    "            'medium_corner': 'medium_corner_speed',\n",
    "            'high_corner': 'high_corner_speed',\n",
    "            'straight': 'avg_speed_full_throttle',\n",
    "            'throttle_usage': 'pct_full_throttle'\n",
    "        }\n",
    "        \n",
    "        for idx, row in testing_features.iterrows():\n",
    "            driver = row['driver_number']\n",
    "            score = {'driver_number': driver}\n",
    "            \n",
    "            for metric_name, feature_col in features.items():\n",
    "                if feature_col in testing_features.columns and pd.notna(row[feature_col]):\n",
    "                    mean_val = testing_features[feature_col].mean()\n",
    "                    std_val = testing_features[feature_col].std()\n",
    "                    \n",
    "                    if std_val > 0:\n",
    "                        z_score = (row[feature_col] - mean_val) / std_val\n",
    "                        score[f'{metric_name}_score'] = z_score\n",
    "                    else:\n",
    "                        score[f'{metric_name}_score'] = 0\n",
    "                else:\n",
    "                    score[f'{metric_name}_score'] = np.nan\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "# TEST: Compare Scoring Methods\n",
    "\n",
    "print(\"\\nLoading 2024 testing features...\")\n",
    "testing_2024 = pd.read_parquet('../data/processed/testing_files/2024_testing_features.parquet')\n",
    "print(f\"ðŸŸ¢ Loaded {len(testing_2024)} driver-sessions\")\n",
    "\n",
    "# Use Practice 3\n",
    "day3_features = testing_2024[testing_2024['session_type'] == 'Practice 3']\n",
    "if len(day3_features) == 0:\n",
    "    day3_features = testing_2024[testing_2024['session_type'] == 'Practice 1']\n",
    "\n",
    "print(f\"\\nScoring {len(day3_features)} drivers...\")\n",
    "\n",
    "methods = {\n",
    "    'Absolute Difference': AbsoluteDifferenceScoring(),\n",
    "    'Ranking': RankingScoring(),\n",
    "    'Quantile': QuantileScoring(),\n",
    "    'Z-Score': ZScoreScoring()\n",
    "}\n",
    "\n",
    "scores_by_method = {}\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    scores = method.score_drivers(day3_features)  # FIXED: score_drivers not score_teams\n",
    "    scores_by_method[method_name] = scores\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    \n",
    "    ver_row = scores[scores['driver_number'] == '1']\n",
    "    if len(ver_row) > 0:\n",
    "        print(f\"  VER medium corners: {ver_row['medium_corner_score'].values[0]:.2f}\")\n",
    "    \n",
    "    scores_clean = scores.dropna(subset=['medium_corner_score'])\n",
    "    \n",
    "    if method_name == 'Ranking':\n",
    "        top3 = scores_clean.nsmallest(3, 'medium_corner_score')\n",
    "    else:\n",
    "        top3 = scores_clean.nlargest(3, 'medium_corner_score')\n",
    "    \n",
    "    print(f\"  Top 3 medium corners:\")\n",
    "    for idx, row in top3.iterrows():\n",
    "        print(f\"    Driver {row['driver_number']:2} - {row['medium_corner_score']:.2f}\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ All scoring methods tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ce195",
   "metadata": {},
   "source": [
    "#### INVESTIGATE: Why is LEC +40 km/h?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280484d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INVESTIGATION: LEC Medium Corner Anomaly\n",
      "================================================================================\n",
      "\n",
      "Raw medium corner speeds (Day 3):\n",
      "driver_number  medium_corner_speed\n",
      "           77           151.365280\n",
      "           22           150.871458\n",
      "           30           150.715894\n",
      "           24           150.219497\n",
      "           44           149.936309\n",
      "           20           149.866616\n",
      "           27           149.610068\n",
      "            1           149.534772\n",
      "           11           149.268857\n",
      "           43           148.897048\n",
      "            4           148.631020\n",
      "           14           148.610712\n",
      "           55           148.594428\n",
      "           18           148.541777\n",
      "           10           148.445143\n",
      "           31           148.237620\n",
      "           63           148.157541\n",
      "           23           148.122323\n",
      "           81           147.953032\n",
      "           16           147.854226\n",
      "\n",
      "Median: 148.76 km/h\n",
      "Mean: 149.17 km/h\n",
      "Std: 1.04 km/h\n",
      "\n",
      "LEC's values:\n",
      "  Medium corner: 147.85 km/h\n",
      "  Slow corner: 89.70 km/h\n",
      "  High corner: 225.40 km/h\n",
      "  Straight: 251.87 km/h\n",
      "  Clean laps: 16\n",
      "\n",
      "Missing data: 0/20 drivers\n",
      "\n",
      "================================================================================\n",
      "Hypothesis: Outlier or missing data issue?\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INVESTIGATION: LEC Medium Corner Anomaly\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check raw values\n",
    "print(\"\\nRaw medium corner speeds (Day 3):\")\n",
    "medium_speeds = day3_features[['driver_number', 'medium_corner_speed']].sort_values(\n",
    "    'medium_corner_speed', ascending=False\n",
    ")\n",
    "print(medium_speeds.to_string(index=False))\n",
    "\n",
    "print(f\"\\nMedian: {day3_features['medium_corner_speed'].median():.2f} km/h\")\n",
    "print(f\"Mean: {day3_features['medium_corner_speed'].mean():.2f} km/h\")\n",
    "print(f\"Std: {day3_features['medium_corner_speed'].std():.2f} km/h\")\n",
    "\n",
    "# Check if LEC value is real or data issue\n",
    "lec_row = day3_features[day3_features['driver_number'] == '16']\n",
    "if len(lec_row) > 0:\n",
    "    print(f\"\\nLEC's values:\")\n",
    "    print(f\"  Medium corner: {lec_row['medium_corner_speed'].values[0]:.2f} km/h\")\n",
    "    print(f\"  Slow corner: {lec_row['slow_corner_speed'].values[0]:.2f} km/h\")\n",
    "    print(f\"  High corner: {lec_row['high_corner_speed'].values[0]:.2f} km/h\")\n",
    "    print(f\"  Straight: {lec_row['avg_speed_full_throttle'].values[0]:.2f} km/h\")\n",
    "    print(f\"  Clean laps: {lec_row['clean_laps'].values[0]}\")\n",
    "\n",
    "# Check for missing data\n",
    "missing_count = day3_features['medium_corner_speed'].isna().sum()\n",
    "print(f\"\\nMissing data: {missing_count}/{len(day3_features)} drivers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Hypothesis: Outlier or missing data issue?\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b5dc3",
   "metadata": {},
   "source": [
    "### PART 3C: Outlier Handling Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5f360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3C: Test Outlier Handling\n",
      "================================================================================\n",
      "\n",
      "Testing outlier removal on medium corners:\n",
      "Original: 20 drivers\n",
      "  Removed 1 outliers in medium_corner_speed: ['77']\n",
      "    Bounds: [146.6, 150.9]\n",
      "After outlier removal: 19 drivers\n",
      "\n",
      "================================================================================\n",
      "Comparison: Raw vs Robust\n",
      "================================================================================\n",
      "\n",
      "Raw Data (20 drivers):\n",
      "  Top 3 medium corners:\n",
      "    Driver 77 -  +2.60 km/h\n",
      "    Driver 22 -  +2.11 km/h\n",
      "    Driver 30 -  +1.95 km/h\n",
      "\n",
      "Robust Data (19 drivers):\n",
      "  Top 3 medium corners:\n",
      "    Driver 22 -  +2.24 km/h\n",
      "    Driver 30 -  +2.08 km/h\n",
      "    Driver 24 -  +1.59 km/h\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3C: Test Outlier Handling\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "def remove_outliers_mad(df: pd.DataFrame, feature: str, n_mad: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove outliers using Median Absolute Deviation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data\n",
    "    feature : str\n",
    "        Column to check for outliers\n",
    "    n_mad : float\n",
    "        Number of MADs from median to consider outlier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Data with outliers removed\n",
    "    \"\"\"\n",
    "    values = df[feature].dropna()\n",
    "    median = values.median()\n",
    "    mad = median_abs_deviation(values, nan_policy='omit')\n",
    "    \n",
    "    # Define outlier bounds\n",
    "    lower_bound = median - n_mad * mad\n",
    "    upper_bound = median + n_mad * mad\n",
    "    \n",
    "    # Filter\n",
    "    mask = (df[feature] >= lower_bound) & (df[feature] <= upper_bound)\n",
    "    \n",
    "    removed = len(df) - mask.sum()\n",
    "    if removed > 0:\n",
    "        removed_drivers = df[~mask]['driver_number'].tolist()\n",
    "        print(f\"  Removed {removed} outliers in {feature}: {removed_drivers}\")\n",
    "        print(f\"    Bounds: [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "# Test on medium corners\n",
    "print(\"\\nTesting outlier removal on medium corners:\")\n",
    "print(f\"Original: {len(day3_features)} drivers\")\n",
    "\n",
    "day3_robust = remove_outliers_mad(\n",
    "    day3_features.copy(), \n",
    "    'medium_corner_speed',\n",
    "    n_mad=3.0\n",
    ")\n",
    "\n",
    "print(f\"After outlier removal: {len(day3_robust)} drivers\")\n",
    "\n",
    "# Score both versions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison: Raw vs Robust\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, dataset in [('Raw', day3_features), ('Robust', day3_robust)]:\n",
    "    print(f\"\\n{dataset_name} Data ({len(dataset)} drivers):\")\n",
    "    \n",
    "    # Use Absolute Difference scoring\n",
    "    scorer = AbsoluteDifferenceScoring()\n",
    "    scores = scorer.score_drivers(dataset)\n",
    "    \n",
    "    # Show top 3\n",
    "    scores_clean = scores.dropna(subset=['medium_corner_score'])\n",
    "    top3 = scores_clean.nlargest(3, 'medium_corner_score')\n",
    "    \n",
    "    print(f\"  Top 3 medium corners:\")\n",
    "    for idx, row in top3.iterrows():\n",
    "        print(f\"    Driver {row['driver_number']:2} - {row['medium_corner_score']:+6.2f} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2971f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Prediction functions defined\n",
      "ðŸŸ¢ All prediction functions defined\n"
     ]
    }
   ],
   "source": [
    "def predict_race_simple(\n",
    "    driver_scores: pd.DataFrame,\n",
    "    track_chars: dict,\n",
    "    bayesian_priors: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple prediction: weighted sum of scores by track characteristics.\n",
    "    \n",
    "    track_chars should have keys like:\n",
    "      'medium_corner_time_pct', 'slow_corner_time_pct', etc.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in driver_scores.iterrows():\n",
    "        driver_num = row['driver_number']\n",
    "        \n",
    "        # Weighted sum of scores\n",
    "        testing_signal = (\n",
    "            track_chars.get('medium_corner_time_pct', 0.4) * row.get('medium_corner_score', 0) +\n",
    "            track_chars.get('slow_corner_time_pct', 0.2) * row.get('slow_corner_score', 0) +\n",
    "            track_chars.get('high_corner_time_pct', 0.2) * row.get('high_corner_score', 0) +\n",
    "            track_chars.get('straight_time_pct', 0.2) * row.get('straight_score', 0)\n",
    "        )\n",
    "        \n",
    "        # Get Bayesian prior\n",
    "        prior_mu = bayesian_priors[driver_num].mu if driver_num in bayesian_priors else 10.0\n",
    "        \n",
    "        # Combine: 90% prior, 10% testing\n",
    "        final_rating = 0.9 * prior_mu + 0.1 * testing_signal\n",
    "        \n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'rating': final_rating\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(predictions)\n",
    "    df = df.sort_values('rating', ascending=False).reset_index(drop=True)\n",
    "    df['predicted_position'] = range(1, len(df) + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_race_fixed(\n",
    "    driver_scores: pd.DataFrame,\n",
    "    track_chars: dict,\n",
    "    bayesian_priors: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixed prediction with proper z-score normalization.\n",
    "    \"\"\"\n",
    "    # Normalize scores to z-scores\n",
    "    scores_normalized = driver_scores.copy()\n",
    "    \n",
    "    score_cols = ['slow_corner_score', 'medium_corner_score', \n",
    "                  'high_corner_score', 'straight_score']\n",
    "    \n",
    "    for col in score_cols:\n",
    "        if col in scores_normalized.columns:\n",
    "            mean = scores_normalized[col].mean()\n",
    "            std = scores_normalized[col].std()\n",
    "            if std > 0:\n",
    "                scores_normalized[f'{col}_z'] = (scores_normalized[col] - mean) / std\n",
    "            else:\n",
    "                scores_normalized[f'{col}_z'] = 0\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in scores_normalized.iterrows():\n",
    "        driver_num = row['driver_number']\n",
    "        \n",
    "        # Track-weighted z-scores\n",
    "        testing_signal = (\n",
    "            track_chars.get('medium_corner_time_pct', 0.4) * row.get('medium_corner_score_z', 0) +\n",
    "            track_chars.get('slow_corner_time_pct', 0.2) * row.get('slow_corner_score_z', 0) +\n",
    "            track_chars.get('high_corner_time_pct', 0.2) * row.get('high_corner_score_z', 0) +\n",
    "            track_chars.get('straight_time_pct', 0.2) * row.get('straight_score_z', 0)\n",
    "        )\n",
    "        \n",
    "        # Get Bayesian prior\n",
    "        prior_mu = bayesian_priors[driver_num].mu if driver_num in bayesian_priors else 10.0\n",
    "        \n",
    "        # Combine: 90% prior, 10% testing\n",
    "        final_rating = 0.9 * (21 - prior_mu) + 0.1 * testing_signal\n",
    "        \n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'rating': final_rating\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(predictions)\n",
    "    df = df.sort_values('rating').reset_index(drop=True)\n",
    "    df['predicted_position'] = range(1, len(df) + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"ðŸŸ¢ Prediction functions defined\")\n",
    "\n",
    "def predict_prior_only(bayesian_priors: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Baseline: Predict using ONLY Bayesian priors, no testing data.\n",
    "    \n",
    "    This is the \"do nothing\" baseline - just rank drivers by championship standings.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for driver_num, prior in bayesian_priors.items():\n",
    "        # Lower Î¼ = better expected position\n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'predicted_position': int(21 - prior.mu)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(predictions)\n",
    "    df = df.sort_values('predicted_position').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"ðŸŸ¢ All prediction functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c8c55",
   "metadata": {},
   "source": [
    "### PART 3D: Simple Prediction Test - Bahrain 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f6632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3D: Predict Bahrain 2024 Race\n",
      "================================================================================\n",
      "\n",
      "Predicting Bahrain 2024 Race...\n",
      "  Removed 1 outliers in medium_corner_speed: ['77']\n",
      "    Bounds: [146.6, 150.9]\n",
      "\n",
      "ðŸŸ¢ Predicted positions for 19 drivers\n",
      "\n",
      "Loading actual Bahrain 2024 race results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '55', '16', '63', '4', '44', '81', '14', '18', '24', '20', '3', '22', '23', '27', '31', '10', '77', '2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: Testing Profile â†’ Race Prediction\n",
      "================================================================================\n",
      "\n",
      "Mean Absolute Error: 3.41 positions\n",
      "Median Error: 2.00 positions\n",
      "\n",
      "Top 10 Predicted vs Actual:\n",
      "Abbreviation  predicted_position  Position  error\n",
      "         NOR                   1       6.0    5.0\n",
      "         VER                   2       1.0    1.0\n",
      "         PIA                   3       8.0    5.0\n",
      "         RUS                   4       5.0    1.0\n",
      "         LEC                   5       4.0    1.0\n",
      "         HAM                   6       7.0    1.0\n",
      "         SAI                   7       3.0    4.0\n",
      "         ALB                   8      15.0    7.0\n",
      "         ALO                   9       9.0    0.0\n",
      "         MAG                  10      12.0    2.0\n",
      "\n",
      "Biggest Prediction Errors:\n",
      "Abbreviation  predicted_position  Position  error\n",
      "         PER                  14       2.0   12.0\n",
      "         STR                  18      10.0    8.0\n",
      "         ALB                   8      15.0    7.0\n",
      "         NOR                   1       6.0    5.0\n",
      "         PIA                   3       8.0    5.0\n",
      "\n",
      "================================================================================\n",
      "Baseline: Random Guess\n",
      "================================================================================\n",
      "Random MAE: 7.59 positions\n",
      "Our MAE: 3.41 positions\n",
      "Improvement: 4.18 positions (55.0%)\n",
      "\n",
      "ðŸŸ¢ MAE < 5 - Concept works! Ready to optimize.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3D: Predict Bahrain 2024 Race\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPredicting Bahrain 2024 Race...\")\n",
    "\n",
    "# Define simple track characteristics (placeholder)\n",
    "# In real implementation, extract from track database\n",
    "bahrain_chars = {\n",
    "    'slow_corner_time_pct': 0.2,\n",
    "    'medium_corner_time_pct': 0.4,\n",
    "    'high_corner_time_pct': 0.2,\n",
    "    'straight_time_pct': 0.2\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Use Robust + Absolute Difference scoring\n",
    "scorer = AbsoluteDifferenceScoring()\n",
    "day3_robust = remove_outliers_mad(day3_features.copy(), 'medium_corner_speed', n_mad=3.0)\n",
    "driver_scores = scorer.score_drivers(day3_robust)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_race_simple(\n",
    "    driver_scores,\n",
    "    bahrain_chars,\n",
    "    priors\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŸ¢ Predicted positions for {len(predictions)} drivers\")\n",
    "\n",
    "# Load actual Bahrain 2024 race results\n",
    "print(\"\\nLoading actual Bahrain 2024 race results...\")\n",
    "bahrain_2024_race = fastf1.get_session(2024, 'Bahrain', 'Race')\n",
    "bahrain_2024_race.load()\n",
    "\n",
    "actual_results = bahrain_2024_race.results[['DriverNumber', 'Position', 'Abbreviation']].copy()\n",
    "actual_results = actual_results[actual_results['Position'].notna()]\n",
    "actual_results['DriverNumber'] = actual_results['DriverNumber'].astype(str)\n",
    "\n",
    "# Merge predictions with actuals\n",
    "comparison = predictions.merge(\n",
    "    actual_results,\n",
    "    left_on='driver_number',\n",
    "    right_on='DriverNumber',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "comparison['error'] = abs(comparison['predicted_position'] - comparison['Position'])\n",
    "\n",
    "# Calculate MAE\n",
    "mae = comparison['error'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS: Testing Profile â†’ Race Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMean Absolute Error: {mae:.2f} positions\")\n",
    "print(f\"Median Error: {comparison['error'].median():.2f} positions\")\n",
    "\n",
    "# Show top 10 comparison\n",
    "print(f\"\\nTop 10 Predicted vs Actual:\")\n",
    "print(comparison.nsmallest(10, 'predicted_position')[\n",
    "    ['Abbreviation', 'predicted_position', 'Position', 'error']\n",
    "].to_string(index=False))\n",
    "\n",
    "# Show biggest errors\n",
    "print(f\"\\nBiggest Prediction Errors:\")\n",
    "print(comparison.nlargest(5, 'error')[\n",
    "    ['Abbreviation', 'predicted_position', 'Position', 'error']\n",
    "].to_string(index=False))\n",
    "\n",
    "# Baseline comparison\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline: Random Guess\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random_positions = list(range(1, len(comparison) + 1))\n",
    "random.shuffle(random_positions)\n",
    "comparison['random_prediction'] = random_positions\n",
    "comparison['random_error'] = abs(comparison['random_prediction'] - comparison['Position'])\n",
    "random_mae = comparison['random_error'].mean()\n",
    "\n",
    "print(f\"Random MAE: {random_mae:.2f} positions\")\n",
    "print(f\"Our MAE: {mae:.2f} positions\")\n",
    "print(f\"Improvement: {random_mae - mae:.2f} positions ({(random_mae - mae)/random_mae*100:.1f}%)\")\n",
    "\n",
    "if mae < 5:\n",
    "    print(\"\\nðŸŸ¢ MAE < 5 - Concept works! Ready to optimize.\")\n",
    "elif mae < 7:\n",
    "    print(\"\\nâš ï¸  MAE 5-7 - Promising, needs refinement.\")\n",
    "else:\n",
    "    print(\"\\nðŸ”´ MAE > 7 - Concept needs rethinking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cfc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIXING: Proper Normalization\n",
      "================================================================================\n",
      "\n",
      "Re-predicting with FIXED normalization...\n",
      "\n",
      "================================================================================\n",
      "FIXED RESULTS\n",
      "================================================================================\n",
      "\n",
      "MAE: 3.35 positions\n",
      "\n",
      "Top 10 Predicted vs Actual:\n",
      "driver_number  predicted_position  Position  error\n",
      "            1                   1       1.0    0.0\n",
      "            4                   2       6.0    4.0\n",
      "           81                   3       8.0    5.0\n",
      "           63                   4       5.0    1.0\n",
      "           16                   5       4.0    1.0\n",
      "           44                   6       7.0    1.0\n",
      "           55                   7       3.0    4.0\n",
      "           14                   8       9.0    1.0\n",
      "           23                   9      15.0    6.0\n",
      "           20                  10      12.0    2.0\n",
      "\n",
      "Who did I predict P1?\n",
      "  Driver 1 (actual: P1)\n",
      "\n",
      "Who actually won?\n",
      "  Driver 1 (I predicted: P1)\n"
     ]
    }
   ],
   "source": [
    "# FIX: Proper Prediction Function\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIXING: Proper Normalization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_race_fixed(\n",
    "    driver_scores: pd.DataFrame,\n",
    "    track_chars: dict,\n",
    "    bayesian_priors: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixed prediction with proper normalization.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert scores to z-scores (dimensionless)\n",
    "    2. Weight by track characteristics\n",
    "    3. Combine with Bayesian priors (90% prior, 10% testing)\n",
    "    \"\"\"\n",
    "    # Normalize all scores to z-scores\n",
    "    features = ['slow_corner_score', 'medium_corner_score', \n",
    "                'high_corner_score', 'straight_score']\n",
    "    \n",
    "    scores_normalized = driver_scores.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        mean = scores_normalized[feature].mean()\n",
    "        std = scores_normalized[feature].std()\n",
    "        \n",
    "        if std > 0:\n",
    "            scores_normalized[f'{feature}_z'] = (scores_normalized[feature] - mean) / std\n",
    "        else:\n",
    "            scores_normalized[f'{feature}_z'] = 0\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in scores_normalized.iterrows():\n",
    "        driver_num = row['driver_number']\n",
    "        \n",
    "        # Track-weighted performance (now dimensionless z-scores)\n",
    "        testing_signal = (\n",
    "            track_chars['medium_corner_time_pct'] * row['medium_corner_score_z'] +\n",
    "            track_chars['slow_corner_time_pct'] * row['slow_corner_score_z'] +\n",
    "            track_chars['high_corner_time_pct'] * row['high_corner_score_z'] +\n",
    "            track_chars['straight_time_pct'] * row['straight_score_z']\n",
    "        )\n",
    "        \n",
    "        # Get Bayesian prior (this is the MAIN signal)\n",
    "        if driver_num in bayesian_priors:\n",
    "            prior_mu = bayesian_priors[driver_num].mu\n",
    "        else:\n",
    "            prior_mu = 10\n",
    "        \n",
    "        # Combine: 90% prior, 10% testing (testing is unreliable!)\n",
    "        combined_rating = 0.9 * prior_mu + 0.1 * testing_signal\n",
    "        \n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'testing_signal': testing_signal,\n",
    "            'prior_mu': prior_mu,\n",
    "            'combined_rating': combined_rating\n",
    "        })\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df = pred_df.sort_values('combined_rating', ascending=False)\n",
    "    pred_df['predicted_position'] = range(1, len(pred_df) + 1)\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "\n",
    "# RE-TEST: With Fixed Prediction\n",
    "\n",
    "print(\"\\nRe-predicting with FIXED normalization...\")\n",
    "\n",
    "predictions_fixed = predict_race_fixed(\n",
    "    driver_scores,\n",
    "    bahrain_chars,\n",
    "    priors\n",
    ")\n",
    "\n",
    "# Merge with actuals\n",
    "comparison_fixed = predictions_fixed.merge(\n",
    "    actual_results,\n",
    "    left_on='driver_number',\n",
    "    right_on='DriverNumber',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "comparison_fixed['error'] = abs(comparison_fixed['predicted_position'] - comparison_fixed['Position'])\n",
    "mae_fixed = comparison_fixed['error'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIXED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMAE: {mae_fixed:.2f} positions\")\n",
    "\n",
    "print(f\"\\nTop 10 Predicted vs Actual:\")\n",
    "print(comparison_fixed.nsmallest(10, 'predicted_position')[\n",
    "    ['driver_number', 'predicted_position', 'Position', 'error']\n",
    "].to_string(index=False))\n",
    "\n",
    "print(f\"\\nWho did I predict P1?\")\n",
    "winner_pred = comparison_fixed[comparison_fixed['predicted_position'] == 1]\n",
    "print(f\"  Driver {winner_pred['driver_number'].values[0]} (actual: P{winner_pred['Position'].values[0]:.0f})\")\n",
    "\n",
    "print(f\"\\nWho actually won?\")\n",
    "actual_winner = comparison_fixed[comparison_fixed['Position'] == 1]\n",
    "print(f\"  Driver {actual_winner['driver_number'].values[0]} (I predicted: P{actual_winner['predicted_position'].values[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4838b",
   "metadata": {},
   "source": [
    "I used FUTURE data (2025) to predict PAST (2024).\n",
    "Why it works well:\n",
    "\n",
    "NOR won 2025 championship â†’ I predicted him P1 in 2024\n",
    "VER 2nd in 2025 â†’ I predicted him P2 in 2024\n",
    "But in reality: VER won 2024 championship, NOR was P2!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10f20cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CRITICAL FIX: Temporal Validation\n",
      "================================================================================\n",
      "\n",
      "2023 Championship-based priors:\n",
      "Top tier:\n",
      "  VER: Î¼=20, Ïƒ=3\n",
      "  PER: Î¼=16, Ïƒ=4\n",
      "\n",
      "McLaren in 2023 (before breakthrough):\n",
      "  NOR: Î¼=15, Ïƒ=4\n",
      "  PIA: Î¼=12, Ïƒ=5\n",
      "\n",
      "================================================================================\n",
      "PROPER TEMPORAL VALIDATION\n",
      "================================================================================\n",
      "\n",
      "MAE with 2023 priors: 2.88 positions\n",
      "MAE with 2025 priors (cheating): 3.35 positions\n",
      "Difference: -0.47 positions\n",
      "\n",
      "Top 10 Predicted vs Actual:\n",
      "driver_number  predicted_position  Position  error\n",
      "            1                   1       1.0    0.0\n",
      "           11                   2       2.0    0.0\n",
      "           44                   3       7.0    4.0\n",
      "            4                   4       6.0    2.0\n",
      "           55                   5       3.0    2.0\n",
      "           16                   6       4.0    2.0\n",
      "           14                   7       9.0    2.0\n",
      "           63                   8       5.0    3.0\n",
      "           81                   9       8.0    1.0\n",
      "           23                  12      15.0    3.0\n",
      "\n",
      "Who did I predict P1?\n",
      "  Driver 1 (actual: P1)\n",
      "\n",
      "Actual winner:\n",
      "  Driver 1 (I predicted: P1)\n",
      "\n",
      "ðŸŸ¢ MAE < 5 with proper validation - System works!\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL FIX: Use 2023 Standings as Priors for 2024\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITICAL FIX: Temporal Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def initialize_2023_standings_priors() -> Dict[str, DriverPrior]:\n",
    "    \"\"\"\n",
    "    2023 Final Championship Standings (correct priors for 2024).\n",
    "    \n",
    "    2023 Results:\n",
    "    1. VER - 575 pts (dominant)\n",
    "    2. PER - 285 pts\n",
    "    3. HAM - 234 pts\n",
    "    4. ALO - 206 pts\n",
    "    5. SAI - 200 pts\n",
    "    6. RUS - 175 pts\n",
    "    7. LEC - 206 pts\n",
    "    8. NOR - 205 pts\n",
    "    9. PIA - 97 pts (rookie)\n",
    "    ...\n",
    "    \"\"\"\n",
    "    priors_2023 = {\n",
    "        # Top tier 2023\n",
    "        '1': DriverPrior('1', 'VER', 'Red Bull Racing', 'top', 'elite', mu=20, sigma=3),  # Dominant\n",
    "        '11': DriverPrior('11', 'PER', 'Red Bull Racing', 'top', 'experienced', mu=16, sigma=4),\n",
    "        \n",
    "        # Upper midfield 2023\n",
    "        '44': DriverPrior('44', 'HAM', 'Mercedes', 'midfield', 'elite', mu=15, sigma=4),\n",
    "        '14': DriverPrior('14', 'ALO', 'Aston Martin', 'midfield', 'elite', mu=15, sigma=4),\n",
    "        '55': DriverPrior('55', 'SAI', 'Ferrari', 'midfield', 'experienced', mu=15, sigma=4),\n",
    "        '63': DriverPrior('63', 'RUS', 'Mercedes', 'midfield', 'experienced', mu=14, sigma=4),\n",
    "        '16': DriverPrior('16', 'LEC', 'Ferrari', 'midfield', 'elite', mu=15, sigma=4),\n",
    "        '4': DriverPrior('4', 'NOR', 'McLaren', 'midfield', 'experienced', mu=15, sigma=4),\n",
    "        \n",
    "        # Promising 2023\n",
    "        '81': DriverPrior('81', 'PIA', 'McLaren', 'midfield', 'rookie', mu=12, sigma=5),  # Strong rookie\n",
    "        \n",
    "        # Lower midfield 2023\n",
    "        '10': DriverPrior('10', 'GAS', 'Alpine', 'backmarker', 'experienced', mu=10, sigma=5),\n",
    "        '23': DriverPrior('23', 'ALB', 'Williams', 'backmarker', 'experienced', mu=10, sigma=5),\n",
    "        '18': DriverPrior('18', 'STR', 'Aston Martin', 'backmarker', 'experienced', mu=9, sigma=5),\n",
    "        '31': DriverPrior('31', 'OCO', 'Alpine', 'backmarker', 'experienced', mu=9, sigma=5),\n",
    "        '20': DriverPrior('20', 'MAG', 'Haas', 'backmarker', 'experienced', mu=8, sigma=6),\n",
    "        '27': DriverPrior('27', 'HUL', 'Haas', 'backmarker', 'experienced', mu=8, sigma=6),\n",
    "        '22': DriverPrior('22', 'TSU', 'AlphaTauri', 'backmarker', 'experienced', mu=8, sigma=6),\n",
    "        '24': DriverPrior('24', 'ZHO', 'Alfa Romeo', 'backmarker', 'experienced', mu=7, sigma=6),\n",
    "        '77': DriverPrior('77', 'BOT', 'Alfa Romeo', 'backmarker', 'experienced', mu=8, sigma=6),\n",
    "        '3': DriverPrior('3', 'RIC', 'AlphaTauri', 'backmarker', 'experienced', mu=9, sigma=5),\n",
    "        '2': DriverPrior('2', 'SAR', 'Williams', 'backmarker', 'rookie', mu=6, sigma=7),\n",
    "    }\n",
    "    \n",
    "    return priors_2023\n",
    "\n",
    "# Initialize correct priors\n",
    "priors_2023 = initialize_2023_standings_priors()\n",
    "\n",
    "print(\"\\n2023 Championship-based priors:\")\n",
    "print(\"Top tier:\")\n",
    "for num in ['1', '11']:\n",
    "    p = priors_2023[num]\n",
    "    print(f\"  {p.driver_code}: Î¼={p.mu}, Ïƒ={p.sigma}\")\n",
    "\n",
    "print(\"\\nMcLaren in 2023 (before breakthrough):\")\n",
    "for num in ['4', '81']:\n",
    "    if num in priors_2023:\n",
    "        p = priors_2023[num]\n",
    "        print(f\"  {p.driver_code}: Î¼={p.mu}, Ïƒ={p.sigma}\")\n",
    "\n",
    "# Re-predict with correct temporal priors\n",
    "predictions_temporal = predict_race_fixed(\n",
    "    driver_scores,\n",
    "    bahrain_chars,\n",
    "    priors_2023\n",
    ")\n",
    "\n",
    "comparison_temporal = predictions_temporal.merge(\n",
    "    actual_results,\n",
    "    left_on='driver_number',\n",
    "    right_on='DriverNumber',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "comparison_temporal['error'] = abs(comparison_temporal['predicted_position'] - comparison_temporal['Position'])\n",
    "mae_temporal = comparison_temporal['error'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROPER TEMPORAL VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMAE with 2023 priors: {mae_temporal:.2f} positions\")\n",
    "print(f\"MAE with 2025 priors (cheating): {mae_fixed:.2f} positions\")\n",
    "print(f\"Difference: {mae_temporal - mae_fixed:+.2f} positions\")\n",
    "\n",
    "print(f\"\\nTop 10 Predicted vs Actual:\")\n",
    "print(comparison_temporal.nsmallest(10, 'predicted_position')[\n",
    "    ['driver_number', 'predicted_position', 'Position', 'error']\n",
    "].to_string(index=False))\n",
    "\n",
    "print(f\"\\nWho did I predict P1?\")\n",
    "winner_pred = comparison_temporal[comparison_temporal['predicted_position'] == 1]\n",
    "if len(winner_pred) > 0:\n",
    "    print(f\"  Driver {winner_pred['driver_number'].values[0]} (actual: P{winner_pred['Position'].values[0]:.0f})\")\n",
    "\n",
    "print(f\"\\nActual winner:\")\n",
    "actual_winner = comparison_temporal[comparison_temporal['Position'] == 1]\n",
    "if len(actual_winner) > 0:\n",
    "    print(f\"  Driver {actual_winner['driver_number'].values[0]} (I predicted: P{actual_winner['predicted_position'].values[0]})\")\n",
    "\n",
    "if mae_temporal < 5:\n",
    "    print(f\"\\nðŸŸ¢ MAE < 5 with proper validation - System works!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  MAE {mae_temporal:.1f} with proper validation - Needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ca1c2",
   "metadata": {},
   "source": [
    "The Hidden Problem\n",
    "MAE 2.61 is suspiciously perfect:\n",
    "\n",
    "VER P1 â†’ Actual P1 âœ… (Error: 0)<br>\n",
    "PER P2 â†’ Actual P2 âœ… (Error: 0)<br>\n",
    "NOR P6 â†’ Actual P6 âœ… (Error: 0)<br>\n",
    "PIA P8 â†’ Actual P8 âœ… (Error: 0)<br>\n",
    "\n",
    "But here's the issue:\n",
    "We're weighting 90% prior, 10% testing. That means we're basically just predicting based on 2023 championship standings, and testing data barely matters!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657c8d6",
   "metadata": {},
   "source": [
    "### CRITICAL TEST: Does Testing Data Add Value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff793e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ABLATION TEST: Prior Only vs Prior + Testing\n",
      "================================================================================\n",
      "\n",
      "Test 1: Prior Only (2023 standings)\n",
      "MAE (Prior Only): 2.60\n",
      "MAE (90% Prior + 10% Testing): 2.88\n",
      "\n",
      "Test 3: Balanced (50% Prior + 50% Testing)\n",
      "MAE (50/50): 2.65\n",
      "\n",
      "Test 4: Testing-Heavy (10% Prior + 90% Testing)\n",
      "MAE (10% Prior + 90% Testing): 3.06\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: Does Testing Data Help?\n",
      "================================================================================\n",
      "\n",
      "MAE by weighting strategy:\n",
      "  Prior Only (0% testing)        MAE: 2.60  (Î”+0.00)\n",
      "  Balanced (50% prior)           MAE: 2.65  (Î”-0.05)\n",
      "  Conservative (90% prior)       MAE: 2.88  (Î”-0.28)\n",
      "  Testing-Heavy (10% prior)      MAE: 3.06  (Î”-0.46)\n",
      "\n",
      "ðŸ”´ Best: Prior Only (0% testing) (MAE: 2.60)\n",
      "\n",
      "âš ï¸  WARNING: Testing data doesn't improve predictions!\n",
      "   Possible reasons:\n",
      "   - Testing too unreliable (sandbagging)\n",
      "   - Track characteristics not discriminative enough\n",
      "   - Need more sophisticated modeling\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION TEST: Prior Only vs Prior + Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_prior_only(priors: dict) -> pd.DataFrame:\n",
    "    \"\"\"Predict using ONLY priors (ignore testing completely).\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for driver_num, prior in priors.items():\n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'combined_rating': prior.mu\n",
    "        })\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df = pred_df.sort_values('combined_rating', ascending=False)\n",
    "    pred_df['predicted_position'] = range(1, len(pred_df) + 1)\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "# Test 1: Prior only (0% testing)\n",
    "print(\"\\nTest 1: Prior Only (2023 standings)\")\n",
    "pred_prior_only = predict_prior_only(priors_2023)\n",
    "comp_prior = pred_prior_only.merge(actual_results, left_on='driver_number', right_on='DriverNumber', how='inner')\n",
    "comp_prior['error'] = abs(comp_prior['predicted_position'] - comp_prior['Position'])\n",
    "mae_prior = comp_prior['error'].mean()\n",
    "\n",
    "print(f\"MAE (Prior Only): {mae_prior:.2f}\")\n",
    "\n",
    "# Test 2: Current system (90% prior, 10% testing)\n",
    "print(f\"MAE (90% Prior + 10% Testing): {mae_temporal:.2f}\")\n",
    "\n",
    "# Test 3: Balanced (50% prior, 50% testing)\n",
    "def predict_balanced(\n",
    "    driver_scores: pd.DataFrame,\n",
    "    track_chars: dict,\n",
    "    bayesian_priors: dict,\n",
    "    prior_weight: float = 0.5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Predict with adjustable weights.\"\"\"\n",
    "    scores_normalized = driver_scores.copy()\n",
    "    \n",
    "    features = ['slow_corner_score', 'medium_corner_score', 'high_corner_score', 'straight_score']\n",
    "    \n",
    "    for feature in features:\n",
    "        mean = scores_normalized[feature].mean()\n",
    "        std = scores_normalized[feature].std()\n",
    "        if std > 0:\n",
    "            scores_normalized[f'{feature}_z'] = (scores_normalized[feature] - mean) / std\n",
    "        else:\n",
    "            scores_normalized[f'{feature}_z'] = 0\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in scores_normalized.iterrows():\n",
    "        driver_num = row['driver_number']\n",
    "        \n",
    "        testing_signal = (\n",
    "            track_chars['medium_corner_time_pct'] * row['medium_corner_score_z'] +\n",
    "            track_chars['slow_corner_time_pct'] * row['slow_corner_score_z'] +\n",
    "            track_chars['high_corner_time_pct'] * row['high_corner_score_z'] +\n",
    "            track_chars['straight_time_pct'] * row['straight_score_z']\n",
    "        )\n",
    "        \n",
    "        if driver_num in bayesian_priors:\n",
    "            prior_mu = bayesian_priors[driver_num].mu\n",
    "        else:\n",
    "            prior_mu = 10\n",
    "        \n",
    "        # Adjustable weighting\n",
    "        combined_rating = prior_weight * prior_mu + (1 - prior_weight) * testing_signal\n",
    "        \n",
    "        predictions.append({\n",
    "            'driver_number': driver_num,\n",
    "            'combined_rating': combined_rating\n",
    "        })\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df = pred_df.sort_values('combined_rating', ascending=False)\n",
    "    pred_df['predicted_position'] = range(1, len(pred_df) + 1)\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "print(\"\\nTest 3: Balanced (50% Prior + 50% Testing)\")\n",
    "pred_balanced = predict_balanced(driver_scores, bahrain_chars, priors_2023, prior_weight=0.5)\n",
    "comp_balanced = pred_balanced.merge(actual_results, left_on='driver_number', right_on='DriverNumber', how='inner')\n",
    "comp_balanced['error'] = abs(comp_balanced['predicted_position'] - comp_balanced['Position'])\n",
    "mae_balanced = comp_balanced['error'].mean()\n",
    "\n",
    "print(f\"MAE (50/50): {mae_balanced:.2f}\")\n",
    "\n",
    "# Test 4: Testing-heavy (10% prior, 90% testing)\n",
    "print(\"\\nTest 4: Testing-Heavy (10% Prior + 90% Testing)\")\n",
    "pred_testing = predict_balanced(driver_scores, bahrain_chars, priors_2023, prior_weight=0.1)\n",
    "comp_testing = pred_testing.merge(actual_results, left_on='driver_number', right_on='DriverNumber', how='inner')\n",
    "comp_testing['error'] = abs(comp_testing['predicted_position'] - comp_testing['Position'])\n",
    "mae_testing = comp_testing['error'].mean()\n",
    "\n",
    "print(f\"MAE (10% Prior + 90% Testing): {mae_testing:.2f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Does Testing Data Help?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = [\n",
    "    ('Prior Only (0% testing)', mae_prior),\n",
    "    ('Conservative (90% prior)', mae_temporal),\n",
    "    ('Balanced (50% prior)', mae_balanced),\n",
    "    ('Testing-Heavy (10% prior)', mae_testing),\n",
    "]\n",
    "\n",
    "print(\"\\nMAE by weighting strategy:\")\n",
    "for name, mae in sorted(results, key=lambda x: x[1]):\n",
    "    improvement = mae_prior - mae\n",
    "    print(f\"  {name:30} MAE: {mae:.2f}  (Î”{improvement:+.2f})\")\n",
    "\n",
    "best_method = min(results, key=lambda x: x[1])\n",
    "print(f\"\\n{'ðŸŸ¢' if best_method[1] < mae_prior else 'ðŸ”´'} Best: {best_method[0]} (MAE: {best_method[1]:.2f})\")\n",
    "\n",
    "if best_method[0] == 'Prior Only (0% testing)':\n",
    "    print(\"\\nâš ï¸  WARNING: Testing data doesn't improve predictions!\")\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   - Testing too unreliable (sandbagging)\")\n",
    "    print(\"   - Track characteristics not discriminative enough\")\n",
    "    print(\"   - Need more sophisticated modeling\")\n",
    "else:\n",
    "    print(f\"\\nðŸŸ¢ Testing data improves MAE by {mae_prior - best_method[1]:.2f} positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a1ed3",
   "metadata": {},
   "source": [
    "## Key Insight: Testing Data in Stable vs Reset Regulations\n",
    "\n",
    "### What I Learned\n",
    "\n",
    "Testing data **hurts** predictions in stable regulations (2024):\n",
    "- Prior only: 2.60 MAE\n",
    "- With testing: 2.88 MAE (11% worse)\n",
    "\n",
    "**Why?** In stable regulations, championship standings are extremely predictive. \n",
    "Teams know their pecking order. Testing adds noise because:\n",
    "\n",
    "1. **Sandbagging** - Teams hide true pace (especially top teams)\n",
    "2. **Program variance** - Long runs vs quali sims vs reliability tests\n",
    "3. **Strategic opacity** - Can't tell fuel loads, engine modes, tire compounds\n",
    "\n",
    "### What Testing CAN Tell Us\n",
    "\n",
    "Testing is poor for **position prediction** but valuable for **car characteristics**:\n",
    "\n",
    "Good signals:\n",
    "- Slow corner performance (mechanical grip)\n",
    "- Fast corner performance (aerodynamic efficiency)\n",
    "- Straight-line speed (power unit, drag)\n",
    "- Relative strengths (cornering god vs straight-line monster)\n",
    "\n",
    "Bad signals:\n",
    "- Absolute lap times (sandbagging)\n",
    "- Direct position prediction (hidden pace)\n",
    "- Race pace simulation (too many unknowns)\n",
    "\n",
    "### The 2026 Difference\n",
    "\n",
    "After regulation changes, this flips:\n",
    "\n",
    "| Regulation State | Priors Quality | Testing Value |\n",
    "|-----------------|----------------|---------------|\n",
    "| **Stable (2024)** | Strong (2023 standings) | Low (sandbagging noise) |\n",
    "| **Reset (2026)** | Weak (no valid history) | High (only signal) |\n",
    "\n",
    "In 2026, everyone starts from zero. No one knows the pecking order. \n",
    "Testing becomes the PRIMARY signal because:\n",
    "- Priors are weak (based on organizational capability only)\n",
    "- Teams can't hide fundamental car characteristics\n",
    "- Relative performance visible even with sandbagging\n",
    "\n",
    "### Better Approach for 2026\n",
    "\n",
    "Current approach: `Testing â†’ Positions` âŒ\n",
    "\n",
    "Better approach: `Testing â†’ Car Profile â†’ Track Match â†’ Adjusted Positions` \n",
    "\n",
    "Step 1: Extract car profiles\n",
    "```\n",
    "Red Bull: {slow_corners: -0.2, fast_corners: +0.8, straights: +0.5}\n",
    "Ferrari:  {slow_corners: +0.3, fast_corners: +0.2, straights: +0.9}\n",
    "McLaren:  {slow_corners: +0.5, fast_corners: +0.4, straights: +0.1}\n",
    "```\n",
    "\n",
    "Step 2: Match to track characteristics\n",
    "```\n",
    "Monaco (70% slow corners) â†’ McLaren advantage\n",
    "Monza (60% straights)     â†’ Ferrari advantage\n",
    "Silverstone (balanced)    â†’ Red Bull advantage\n",
    "```\n",
    "\n",
    "Step 3: Adjust priors by track fit\n",
    "```\n",
    "LEC at Monaco: Prior Î¼=14 â†’ Adjusted Î¼=13 (Ferrari weak in slow corners)\n",
    "LEC at Monza:  Prior Î¼=14 â†’ Adjusted Î¼=15 (Ferrari strong on straights)\n",
    "```\n",
    "\n",
    "### Actionable Next Steps\n",
    "\n",
    "For 2024/2025 (stable regulations):\n",
    "- Use priors only (championship standings)\n",
    "- Skip testing data (adds noise)\n",
    "- Focus on sequential learning from race results\n",
    "\n",
    "For 2026 (regulation reset):\n",
    "- Build car profile extraction from testing\n",
    "- Implement sandbagging detection (fuel load proxies, % full throttle)\n",
    "- Create track-characteristic matching system\n",
    "- Weight testing heavily (70-90%) because priors are weak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
