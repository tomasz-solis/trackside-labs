{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Model Validation - Full 2025 Season\n",
    "\n",
    "Compare predictions vs actual results for all 24 races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.predictors.team_predictor import rank_teams_for_track\n",
    "from src.utils.validation import (\n",
    "    analyze_by_stage,\n",
    "    compare_rankings,\n",
    "    confidence_calibration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游릭 Loaded: tracks (24), teams (10), results (24 races)\n"
     ]
    }
   ],
   "source": [
    "loaded = []\n",
    "errors = []\n",
    "\n",
    "# Load tracks\n",
    "try:\n",
    "    track_path = Path('../data/processed/testing_files/track_characteristics/2025_track_characteristics.json')\n",
    "    with open(track_path) as f:\n",
    "        track_data = json.load(f)\n",
    "    all_tracks = track_data.get('tracks', {})\n",
    "    loaded.append(f\"tracks ({len(all_tracks)})\")\n",
    "except FileNotFoundError:\n",
    "    errors.append(\"track characteristics\")\n",
    "    all_tracks = {}\n",
    "\n",
    "# Load cars\n",
    "try:\n",
    "    car_path = Path('../data/processed/testing_files/car_characteristics/2025_car_characteristics.json')\n",
    "    with open(car_path) as f:\n",
    "        car_data = json.load(f)\n",
    "    all_cars = car_data.get('teams', {})\n",
    "    loaded.append(f\"teams ({len(all_cars)})\")\n",
    "except FileNotFoundError:\n",
    "    errors.append(\"car characteristics\")\n",
    "    all_cars = {}\n",
    "\n",
    "# Load actual results\n",
    "try:\n",
    "    results_path = Path('../data/processed/testing_files/validation/2025_qualifying_results.json')\n",
    "    with open(results_path) as f:\n",
    "        actual_results = json.load(f)\n",
    "    loaded.append(f\"results ({actual_results.get('total_races', 0)} races)\")\n",
    "except FileNotFoundError:\n",
    "    errors.append(\"qualifying results\")\n",
    "    actual_results = {}\n",
    "\n",
    "# Print summary\n",
    "if loaded:\n",
    "    print(f\"游릭 Loaded: {', '.join(loaded)}\")\n",
    "if errors:\n",
    "    print(f\"游댮  Missing: {', '.join(errors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "======================================================================\n",
      "  Australian Grand Prix: 3 stages\n",
      "  Chinese Grand Prix: 2 stages\n",
      "  Japanese Grand Prix: 3 stages\n",
      "  Bahrain Grand Prix: 3 stages\n",
      "  Saudi Arabian Grand Prix: 3 stages\n",
      "  Miami Grand Prix: 2 stages\n",
      "  Emilia Romagna Grand Prix: 3 stages\n",
      "  Monaco Grand Prix: 3 stages\n",
      "  Spanish Grand Prix: 3 stages\n",
      "  Canadian Grand Prix: 3 stages\n",
      "  Austrian Grand Prix: 3 stages\n",
      "  British Grand Prix: 3 stages\n",
      "  Belgian Grand Prix: 2 stages\n",
      "  Hungarian Grand Prix: 3 stages\n",
      "  Dutch Grand Prix: 3 stages\n",
      "  Italian Grand Prix: 3 stages\n",
      "  Azerbaijan Grand Prix: 3 stages\n",
      "  Singapore Grand Prix: 3 stages\n",
      "  United States Grand Prix: 2 stages\n",
      "  Mexico City Grand Prix: 3 stages\n",
      "  S칚o Paulo Grand Prix: 2 stages\n",
      "  Las Vegas Grand Prix: 3 stages\n",
      "  Qatar Grand Prix: 2 stages\n",
      "  Abu Dhabi Grand Prix: 3 stages\n",
      "\n",
      "游릭 Generated predictions for 24 races\n"
     ]
    }
   ],
   "source": [
    "all_predictions = {}\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for race_name, race_data in actual_results['races'].items():\n",
    "    \n",
    "    if race_name not in all_tracks:\n",
    "        print(f\"游댮 {race_name}: No track data\")\n",
    "        continue\n",
    "    \n",
    "    track_chars = all_tracks[race_name]\n",
    "    weekend_type = race_data['weekend_type']\n",
    "    \n",
    "    if weekend_type == 'sprint':\n",
    "        stages = [('post_fp1', 'sprint'), ('post_sprint_quali', 'sprint')]\n",
    "    else:\n",
    "        stages = [('post_fp1', 'normal'), ('post_fp2', 'normal'), ('post_fp3', 'normal')]\n",
    "    \n",
    "    race_predictions = {}\n",
    "    \n",
    "    for stage, wtype in stages:\n",
    "        rankings = rank_teams_for_track(all_cars, track_chars, stage, wtype)\n",
    "        \n",
    "        if rankings:\n",
    "            race_predictions[stage] = {\n",
    "                'teams': [team for team, _, _, _ in rankings],\n",
    "                'scores': [score for _, score, _, _ in rankings],\n",
    "                'confidence': rankings[0][2]\n",
    "            }\n",
    "    \n",
    "    all_predictions[race_name] = race_predictions\n",
    "    print(f\"  {race_name}: {len(race_predictions)} stages\")\n",
    "\n",
    "print(f\"\\n游릭 Generated predictions for {len(all_predictions)} races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游릭 Validated 24 races\n"
     ]
    }
   ],
   "source": [
    "validation_results = {}\n",
    "\n",
    "for race_name in all_predictions:\n",
    "    \n",
    "    actual = actual_results['races'][race_name]\n",
    "    actual_teams = [pos['team'] for pos in actual['positions']]\n",
    "    \n",
    "    race_metrics = {}\n",
    "    \n",
    "    for stage, pred_data in all_predictions[race_name].items():\n",
    "        predicted_teams = pred_data['teams']\n",
    "        \n",
    "        metrics = compare_rankings(predicted_teams, actual_teams)\n",
    "        metrics['confidence'] = pred_data['confidence']\n",
    "        \n",
    "        race_metrics[stage] = metrics\n",
    "    \n",
    "    validation_results[race_name] = race_metrics\n",
    "\n",
    "print(f\"游릭 Validated {len(validation_results)} races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Results by Stage\n",
      "======================================================================\n",
      "\n",
      "POST_FP1:\n",
      "  Races: 24\n",
      "  Winner accuracy: 33.3%\n",
      "  Top 3 accuracy: 47.2%\n",
      "  Top 5 accuracy: 50.0%\n",
      "  Spearman correlation: 0.528\n",
      "  Avg positions off: 3.8\n",
      "\n",
      "POST_FP2:\n",
      "  Races: 18\n",
      "  Winner accuracy: 33.3%\n",
      "  Top 3 accuracy: 40.7%\n",
      "  Top 5 accuracy: 45.6%\n",
      "  Spearman correlation: 0.424\n",
      "  Avg positions off: 4.2\n",
      "\n",
      "POST_FP3:\n",
      "  Races: 18\n",
      "  Winner accuracy: 11.1%\n",
      "  Top 3 accuracy: 64.8%\n",
      "  Top 5 accuracy: 66.7%\n",
      "  Spearman correlation: 0.747\n",
      "  Avg positions off: 3.3\n",
      "\n",
      "POST_SPRINT_QUALI:\n",
      "  Races: 6\n",
      "  Winner accuracy: 33.3%\n",
      "  Top 3 accuracy: 38.9%\n",
      "  Top 5 accuracy: 60.0%\n",
      "  Spearman correlation: 0.665\n",
      "  Avg positions off: 3.5\n"
     ]
    }
   ],
   "source": [
    "by_stage = analyze_by_stage(validation_results)\n",
    "\n",
    "print(\"Overall Results by Stage\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stage in ['post_fp1', 'post_fp2', 'post_fp3', 'post_sprint_quali']:\n",
    "    if stage not in by_stage:\n",
    "        continue\n",
    "    \n",
    "    metrics = by_stage[stage]\n",
    "    \n",
    "    print(f\"\\n{stage.upper()}:\")\n",
    "    print(f\"  Races: {metrics['count']}\")\n",
    "    print(f\"  Winner accuracy: {metrics.get('winner_correct', 0):.1%}\")\n",
    "    print(f\"  Top 3 accuracy: {metrics.get('top3_accuracy', 0):.1%}\")\n",
    "    print(f\"  Top 5 accuracy: {metrics.get('top5_accuracy', 0):.1%}\")\n",
    "    print(f\"  Spearman correlation: {metrics.get('spearman', 0):.3f}\")\n",
    "    print(f\"  Avg positions off: {metrics.get('mae_positions', 0):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best vs Worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Predictions (Top 5 accuracy):\n",
      "======================================================================\n",
      "游댮 Chinese Grand Prix             Top5: 80.0%  Spearman: 0.96\n",
      "游댮 Japanese Grand Prix            Top5: 80.0%  Spearman: 0.81\n",
      "游댮 Spanish Grand Prix             Top5: 80.0%  Spearman: 0.87\n",
      "游릭 Canadian Grand Prix            Top5: 80.0%  Spearman: 0.96\n",
      "游댮 British Grand Prix             Top5: 80.0%  Spearman: 0.67\n",
      "\n",
      "Worst Predictions:\n",
      "======================================================================\n",
      "游댮 Singapore Grand Prix           Top5: 60.0%  Spearman: 0.79\n",
      "游릭 United States Grand Prix       Top5: 60.0%  Spearman: 0.52\n",
      "游댮 Las Vegas Grand Prix           Top5: 60.0%  Spearman: 0.79\n",
      "游댮 Qatar Grand Prix               Top5: 60.0%  Spearman: 0.66\n",
      "游댮 S칚o Paulo Grand Prix           Top5: 40.0%  Spearman: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Get final stage for each race\n",
    "final_predictions = {}\n",
    "\n",
    "for race, stages in validation_results.items():\n",
    "    if 'post_fp3' in stages:\n",
    "        final_predictions[race] = stages['post_fp3']\n",
    "    elif 'post_sprint_quali' in stages:\n",
    "        final_predictions[race] = stages['post_sprint_quali']\n",
    "\n",
    "sorted_races = sorted(\n",
    "    final_predictions.items(),\n",
    "    key=lambda x: x[1].get('top5_accuracy', 0),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"Best Predictions (Top 5 accuracy):\")\n",
    "print(\"=\" * 70)\n",
    "for race, metrics in sorted_races[:5]:\n",
    "    winner = '游릭' if metrics.get('winner_correct', 0) == 1.0 else '游댮'\n",
    "    print(f\"{winner} {race:<30} Top5: {metrics.get('top5_accuracy', 0):.1%}  Spearman: {metrics.get('spearman', 0):.2f}\")\n",
    "\n",
    "print(\"\\nWorst Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "for race, metrics in sorted_races[-5:]:\n",
    "    winner = '游릭' if metrics.get('winner_correct', 0) == 1.0 else '游댮'\n",
    "    print(f\"{winner} {race:<30} Top5: {metrics.get('top5_accuracy', 0):.1%}  Spearman: {metrics.get('spearman', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Calibration:\n",
      "======================================================================\n",
      "Brier score: 0.557 (lower = better)\n",
      "\n",
      "0.5-0.7: Accuracy 33.3% (6 races)\n",
      "0.8-1.0: Accuracy 11.1% (18 races)\n"
     ]
    }
   ],
   "source": [
    "conf_predictions = []\n",
    "\n",
    "for race, metrics in final_predictions.items():\n",
    "    conf = metrics.get('confidence', 0.5)\n",
    "    correct = metrics.get('winner_correct', 0) == 1.0\n",
    "    conf_predictions.append((conf, correct))\n",
    "\n",
    "calibration = confidence_calibration(conf_predictions)\n",
    "\n",
    "print(\"Confidence Calibration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Brier score: {calibration['brier_score']:.3f} (lower = better)\\n\")\n",
    "\n",
    "for bin_range, data in calibration['bins'].items():\n",
    "    print(f\"{bin_range}: Accuracy {data['accuracy']:.1%} ({data['count']} races)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游릭 Saved to ../data/processed/testing_files/validation/validation_results.json\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    'overall': by_stage,\n",
    "    'by_race': validation_results,\n",
    "    'calibration': calibration\n",
    "}\n",
    "\n",
    "output_path = Path('../data/processed/testing_files/validation/validation_results.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"游릭 Saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
