{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 21: Complete System Validation (Quali + Race)\n",
        "\n",
        "**Two-stage prediction system:**\n",
        "1. Predict qualifying positions\n",
        "2. After quali, predict race outcomes (position, podium, points, DNF)\n",
        "\n",
        "## Metrics Tracked:\n",
        "**QUALIFYING:**\n",
        "- MAE (same as before)\n",
        "- Calibration (1\u03c3, 2\u03c3)\n",
        "\n",
        "**RACE:**\n",
        "- Position MAE (in positions)\n",
        "- Podium accuracy (%)\n",
        "- Points accuracy (%)\n",
        "- DNF Brier score (lower = better)\n",
        "\n",
        "## Test Season:\n",
        "2024 \u2192 2025 (faster validation, ~30 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEAN ERROR HANDLING\n",
        "import warnings\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('fastf1').setLevel(logging.ERROR)\n",
        "logging.getLogger('requests').setLevel(logging.ERROR)\n",
        "logging.getLogger('urllib3').setLevel(logging.ERROR)\n",
        "logging.getLogger('requests_cache').setLevel(logging.ERROR)\n",
        "sys.tracebacklimit = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fastf1 as ff1\n",
        "from pathlib import Path\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "ff1.Cache.enable_cache('../data/raw/.fastf1_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (using tuned parameters from 20B)\n",
        "TUNED_PARAMS = {\n",
        "    'initial_uncertainty': 0.20,\n",
        "    'min_uncertainty': 0.06,\n",
        "    'measurement_noise': 0.04,\n",
        "    'driver_specific_mins': {\n",
        "        'VER': 0.05, 'RUS': 0.05, 'LEC': 0.06, 'HAM': 0.06,\n",
        "        'NOR': 0.06, 'ALO': 0.06, 'SAI': 0.07, 'PER': 0.10,\n",
        "        'default': 0.07\n",
        "    }\n",
        "}\n",
        "\n",
        "TEST_SEASON = {'from': 2024, 'to': 2025, 'type': 'stable'}\n",
        "TRACKED_DRIVERS = ['VER', 'HAM', 'LEC', 'NOR', 'SAI', 'PER', 'RUS', 'ALO']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Extraction (with Racecraft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_race_results(year, race_name):\n",
        "    \"\"\"Extract both quali AND race results.\"\"\"\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    try:\n",
        "        quali = ff1.get_session(year, race_name, 'Q')\n",
        "        quali.load(laps=False, telemetry=False, weather=False)\n",
        "        \n",
        "        race = ff1.get_session(year, race_name, 'R')\n",
        "        race.load(laps=False, telemetry=False, weather=False)\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        for _, row in quali.results.iterrows():\n",
        "            driver = row['Abbreviation']\n",
        "            quali_pos = row['Position']\n",
        "            \n",
        "            if pd.notna(driver) and pd.notna(quali_pos):\n",
        "                results[driver] = {'quali_pos': int(quali_pos)}\n",
        "        \n",
        "        for _, row in race.results.iterrows():\n",
        "            driver = row['Abbreviation']\n",
        "            race_pos = row['Position']\n",
        "            \n",
        "            dnf = row.dnf if hasattr(row, 'dnf') else False\n",
        "            status = str(row['Status']) if 'Status' in row else ''\n",
        "            if not dnf and status:\n",
        "                dnf = 'Finished' not in status and '+' not in status\n",
        "            \n",
        "            if pd.notna(driver) and driver in results:\n",
        "                if pd.notna(race_pos):\n",
        "                    results[driver]['race_pos'] = int(race_pos)\n",
        "                results[driver]['dnf'] = dnf\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    \u274c Error: {type(e).__name__}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_season_characteristics(year):\n",
        "    \"\"\"Extract prior year characteristics INCLUDING racecraft.\"\"\"\n",
        "    schedule = ff1.get_event_schedule(year)\n",
        "    driver_stats = {}\n",
        "    \n",
        "    print(f\"  Extracting {year} season characteristics...\")\n",
        "    \n",
        "    for _, event in schedule.iterrows():\n",
        "        if event['EventFormat'] != 'conventional':\n",
        "            continue\n",
        "        \n",
        "        race_name = event['EventName']\n",
        "        results = extract_race_results(year, race_name)\n",
        "        if not results:\n",
        "            continue\n",
        "        \n",
        "        for driver, data in results.items():\n",
        "            if driver not in driver_stats:\n",
        "                driver_stats[driver] = {\n",
        "                    'quali_positions': [],\n",
        "                    'race_positions': [],\n",
        "                    'positions_gained': [],\n",
        "                    'dnfs': 0,\n",
        "                    'races': 0\n",
        "                }\n",
        "            \n",
        "            driver_stats[driver]['quali_positions'].append(data['quali_pos'])\n",
        "            driver_stats[driver]['races'] += 1\n",
        "            \n",
        "            if data.get('dnf', False):\n",
        "                driver_stats[driver]['dnfs'] += 1\n",
        "            elif 'race_pos' in data:\n",
        "                driver_stats[driver]['race_positions'].append(data['race_pos'])\n",
        "                positions_gained = data['quali_pos'] - data['race_pos']\n",
        "                driver_stats[driver]['positions_gained'].append(positions_gained)\n",
        "    \n",
        "    # Calculate characteristics\n",
        "    characteristics = {}\n",
        "    for driver, stats in driver_stats.items():\n",
        "        if stats['races'] == 0:\n",
        "            continue\n",
        "        \n",
        "        avg_quali_pos = np.mean(stats['quali_positions'])\n",
        "        avg_pace = 1.0 - (avg_quali_pos - 1) / 19\n",
        "        dnf_rate = stats['dnfs'] / stats['races']\n",
        "        \n",
        "        # Racecraft score from positions gained\n",
        "        if stats['positions_gained']:\n",
        "            avg_gain = np.mean(stats['positions_gained'])\n",
        "            # Scale to 0-1: +3 = 1.0, 0 = 0.5, -3 = 0.0\n",
        "            racecraft_score = 0.5 + (avg_gain / 6.0)\n",
        "            racecraft_score = np.clip(racecraft_score, 0.0, 1.0)\n",
        "        else:\n",
        "            racecraft_score = 0.5\n",
        "        \n",
        "        characteristics[driver] = {\n",
        "            'avg_quali_pace': float(avg_pace),\n",
        "            'dnf_rate': float(dnf_rate),\n",
        "            'racecraft_score': float(racecraft_score),\n",
        "            'races_completed': stats['races']\n",
        "        }\n",
        "    \n",
        "    print(f\"  \u2705 Extracted characteristics for {len(characteristics)} drivers\")\n",
        "    return characteristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Race Prediction System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_race_from_quali(actual_quali_results, driver_priors):\n",
        "    \"\"\"\n",
        "    Predict race outcomes based on actual quali results.\n",
        "    \n",
        "    Returns:\n",
        "    - expected_race_pos\n",
        "    - podium_probability\n",
        "    - points_probability\n",
        "    - dnf_probability\n",
        "    \"\"\"\n",
        "    race_predictions = {}\n",
        "    \n",
        "    for driver, quali_pos in actual_quali_results.items():\n",
        "        if driver not in driver_priors['drivers']:\n",
        "            continue\n",
        "        \n",
        "        driver_data = driver_priors['drivers'][driver]\n",
        "        \n",
        "        # Get racecraft and DNF data\n",
        "        racecraft_score = driver_data.get('racecraft', {}).get('skill_score', 0.5)\n",
        "        dnf_prob = driver_data.get('dnf_risk', {}).get('rate', 0.1)\n",
        "        quali_uncertainty = driver_data['pace']['uncertainty']\n",
        "        \n",
        "        # Racecraft effect: \u00b13 positions\n",
        "        racecraft_delta = (racecraft_score - 0.5) * 6\n",
        "        \n",
        "        # Expected race position\n",
        "        expected_pos = quali_pos - racecraft_delta\n",
        "        expected_pos = np.clip(expected_pos, 1, 20)\n",
        "        \n",
        "        # Race uncertainty\n",
        "        race_uncertainty = np.sqrt(\n",
        "            (quali_uncertainty * 19)**2 + 3**2\n",
        "        )\n",
        "        \n",
        "        # Position probability distribution\n",
        "        positions = np.arange(1, 21)\n",
        "        position_probs = np.exp(-0.5 * ((positions - expected_pos) / race_uncertainty) ** 2)\n",
        "        position_probs = position_probs / position_probs.sum()\n",
        "        \n",
        "        # Adjust for DNF\n",
        "        position_probs_finish = position_probs * (1 - dnf_prob)\n",
        "        position_probs_finish[19] += dnf_prob\n",
        "        \n",
        "        # Calculate probabilities\n",
        "        podium_prob = position_probs_finish[:3].sum()\n",
        "        points_prob = position_probs_finish[:10].sum()\n",
        "        \n",
        "        race_predictions[driver] = {\n",
        "            'quali_pos': quali_pos,\n",
        "            'expected_race_pos': float(expected_pos),\n",
        "            'race_uncertainty': float(race_uncertainty),\n",
        "            'podium_probability': float(podium_prob),\n",
        "            'points_probability': float(points_prob),\n",
        "            'dnf_probability': float(dnf_prob)\n",
        "        }\n",
        "    \n",
        "    return race_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_quali_metrics(predictions, uncertainties, actuals):\n",
        "    \"\"\"Calculate quali prediction metrics.\"\"\"\n",
        "    errors = []\n",
        "    within_1sigma = []\n",
        "    within_2sigma = []\n",
        "    \n",
        "    for driver in predictions:\n",
        "        if driver in actuals:\n",
        "            pred = predictions[driver]\n",
        "            unc = uncertainties[driver]\n",
        "            actual = actuals[driver]\n",
        "            error = abs(pred - actual)\n",
        "            \n",
        "            errors.append(error)\n",
        "            within_1sigma.append(error <= unc)\n",
        "            within_2sigma.append(error <= 2 * unc)\n",
        "    \n",
        "    return {\n",
        "        'mae': np.mean(errors) if errors else None,\n",
        "        'calibration_1sigma': sum(within_1sigma) / len(within_1sigma) * 100 if within_1sigma else None,\n",
        "        'calibration_2sigma': sum(within_2sigma) / len(within_2sigma) * 100 if within_2sigma else None,\n",
        "        'n': len(errors)\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_race_metrics(predictions, actuals):\n",
        "    \"\"\"Calculate race prediction metrics.\"\"\"\n",
        "    position_errors = []\n",
        "    podium_correct = []\n",
        "    points_correct = []\n",
        "    dnf_brier = []\n",
        "    \n",
        "    for driver, pred in predictions.items():\n",
        "        if driver not in actuals:\n",
        "            continue\n",
        "        \n",
        "        actual = actuals[driver]\n",
        "        actual_dnf = actual.get('dnf', False)\n",
        "        \n",
        "        # Position error (finishers only)\n",
        "        if not actual_dnf and 'race_pos' in actual:\n",
        "            error = abs(pred['expected_race_pos'] - actual['race_pos'])\n",
        "            position_errors.append(error)\n",
        "        \n",
        "        # Podium prediction\n",
        "        pred_podium = pred['podium_probability'] > 0.5\n",
        "        actual_podium = (actual.get('race_pos', 21) <= 3) and not actual_dnf\n",
        "        podium_correct.append(pred_podium == actual_podium)\n",
        "        \n",
        "        # Points prediction\n",
        "        pred_points = pred['points_probability'] > 0.5\n",
        "        actual_points = (actual.get('race_pos', 21) <= 10) and not actual_dnf\n",
        "        points_correct.append(pred_points == actual_points)\n",
        "        \n",
        "        # DNF prediction (Brier score)\n",
        "        actual_dnf_binary = 1.0 if actual_dnf else 0.0\n",
        "        brier = (pred['dnf_probability'] - actual_dnf_binary) ** 2\n",
        "        dnf_brier.append(brier)\n",
        "    \n",
        "    return {\n",
        "        'position_mae': np.mean(position_errors) if position_errors else None,\n",
        "        'podium_accuracy': np.mean(podium_correct) * 100 if podium_correct else None,\n",
        "        'points_accuracy': np.mean(points_correct) * 100 if points_correct else None,\n",
        "        'dnf_brier_score': np.mean(dnf_brier) if dnf_brier else None,\n",
        "        'n': len(podium_correct)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuned Bayesian Update (from 20B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tuned_bayesian_update(priors, race_results, season_progress):\n",
        "    \"\"\"Bayesian update with tuning (same as 20B).\"\"\"\n",
        "    posteriors = copy.deepcopy(priors)\n",
        "    posteriors['week'] = priors.get('week', 0) + 1\n",
        "    posteriors['races_seen'] = priors.get('races_seen', 0) + 1\n",
        "    \n",
        "    grid_size = 20\n",
        "    \n",
        "    for driver, result in race_results.items():\n",
        "        if driver not in posteriors['drivers']:\n",
        "            continue\n",
        "        \n",
        "        driver_data = posteriors['drivers'][driver]\n",
        "        races_seen = driver_data.get('races_seen', 0)\n",
        "        \n",
        "        # Adaptive learning\n",
        "        base_alpha = max(0.05, 1.0 / (races_seen + 2))\n",
        "        if 0.4 < season_progress < 0.8:\n",
        "            base_alpha *= 0.7\n",
        "        alpha = base_alpha\n",
        "        \n",
        "        # Update pace\n",
        "        observed_pace = 1.0 - (result['quali_pos'] - 1) / (grid_size - 1)\n",
        "        prior_pace = driver_data['pace']['quali_pace']\n",
        "        new_pace = (1 - alpha) * prior_pace + alpha * observed_pace\n",
        "        \n",
        "        driver_data['pace']['quali_pace'] = float(new_pace)\n",
        "        \n",
        "        # Store history\n",
        "        if 'pace_history' not in driver_data:\n",
        "            driver_data['pace_history'] = []\n",
        "        driver_data['pace_history'].append(float(observed_pace))\n",
        "        \n",
        "        # Update uncertainty with tuning\n",
        "        error = abs(prior_pace - observed_pace)\n",
        "        prior_uncertainty = driver_data['pace']['uncertainty']\n",
        "        \n",
        "        dnf = result.get('dnf', False)\n",
        "        \n",
        "        if dnf:\n",
        "            obs_var = 0.10\n",
        "            n_obs = 0.5\n",
        "        else:\n",
        "            pace_var = np.var(driver_data['pace_history'][-5:]) if len(driver_data['pace_history']) >= 2 else 0.08\n",
        "            obs_var = pace_var + TUNED_PARAMS['measurement_noise']\n",
        "            n_obs = 1.0\n",
        "        \n",
        "        # Bayesian update\n",
        "        prior_var = prior_uncertainty ** 2\n",
        "        posterior_var = 1.0 / (1.0/prior_var + n_obs/obs_var)\n",
        "        new_uncertainty = np.sqrt(posterior_var)\n",
        "        \n",
        "        # Outlier detection\n",
        "        if error > 0.5:\n",
        "            new_uncertainty *= 1.2\n",
        "        \n",
        "        # Driver-specific minimum\n",
        "        driver_min = TUNED_PARAMS['driver_specific_mins'].get(driver, 0.07)\n",
        "        new_uncertainty = max(new_uncertainty, driver_min)\n",
        "        \n",
        "        driver_data['pace']['uncertainty'] = float(new_uncertainty)\n",
        "        driver_data['races_seen'] = races_seen + 1\n",
        "    \n",
        "    return posteriors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPLETE SYSTEM VALIDATION (QUALI + RACE)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTest Season: {TEST_SEASON['from']} \u2192 {TEST_SEASON['to']}\")\n",
        "print(f\"Tracked Drivers: {', '.join(TRACKED_DRIVERS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract prior year characteristics\n",
        "prior_characteristics = extract_season_characteristics(TEST_SEASON['from'])\n",
        "\n",
        "# Initialize priors with racecraft data\n",
        "priors = {\n",
        "    'week': 0,\n",
        "    'season': TEST_SEASON['to'],\n",
        "    'races_seen': 0,\n",
        "    'drivers': {}\n",
        "}\n",
        "\n",
        "for driver_code, char in prior_characteristics.items():\n",
        "    priors['drivers'][driver_code] = {\n",
        "        'pace': {\n",
        "            'quali_pace': char['avg_quali_pace'],\n",
        "            'uncertainty': TUNED_PARAMS['initial_uncertainty'],\n",
        "            'confidence': 'low'\n",
        "        },\n",
        "        'dnf_risk': {'rate': char['dnf_rate']},\n",
        "        'racecraft': {'skill_score': char['racecraft_score']},\n",
        "        'races_seen': 0,\n",
        "        'pace_history': []\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run validation\n",
        "schedule = ff1.get_event_schedule(TEST_SEASON['to'])\n",
        "current_priors = copy.deepcopy(priors)\n",
        "\n",
        "results = {\n",
        "    'quali_metrics_per_race': [],\n",
        "    'race_metrics_per_race': [],\n",
        "    'drivers': {driver: {\n",
        "        'quali_errors': [],\n",
        "        'race_position_errors': [],\n",
        "        'podium_predictions': [],\n",
        "        'points_predictions': []\n",
        "    } for driver in TRACKED_DRIVERS}\n",
        "}\n",
        "\n",
        "week = 0\n",
        "total_races = len([e for _, e in schedule.iterrows() if e['EventFormat'] == 'conventional'])\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"RACE-BY-RACE RESULTS\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _, event in schedule.iterrows():\n",
        "    if event['EventFormat'] != 'conventional':\n",
        "        continue\n",
        "    \n",
        "    week += 1\n",
        "    race_name = event['EventName']\n",
        "    season_progress = week / total_races\n",
        "    \n",
        "    print(f\"\\n\ud83d\udccd Week {week}/{total_races}: {race_name}\")\n",
        "    \n",
        "    # Extract actual results (both quali and race)\n",
        "    actual_results = extract_race_results(TEST_SEASON['to'], race_name)\n",
        "    if not actual_results:\n",
        "        continue\n",
        "    \n",
        "    # STAGE 1: Quali predictions\n",
        "    quali_predictions = {}\n",
        "    quali_uncertainties = {}\n",
        "    quali_actuals = {}\n",
        "    \n",
        "    for driver in TRACKED_DRIVERS:\n",
        "        if driver in current_priors['drivers'] and driver in actual_results:\n",
        "            pred_pace = current_priors['drivers'][driver]['pace']['quali_pace']\n",
        "            unc = current_priors['drivers'][driver]['pace']['uncertainty']\n",
        "            actual_pace = 1.0 - (actual_results[driver]['quali_pos'] - 1) / 19\n",
        "            \n",
        "            quali_predictions[driver] = pred_pace\n",
        "            quali_uncertainties[driver] = unc\n",
        "            quali_actuals[driver] = actual_pace\n",
        "    \n",
        "    quali_metrics = calculate_quali_metrics(quali_predictions, quali_uncertainties, quali_actuals)\n",
        "    \n",
        "    # STAGE 2: Race predictions (based on ACTUAL quali)\n",
        "    actual_quali_only = {d: r['quali_pos'] for d, r in actual_results.items()}\n",
        "    race_predictions = predict_race_from_quali(actual_quali_only, current_priors)\n",
        "    race_metrics = calculate_race_metrics(race_predictions, actual_results)\n",
        "    \n",
        "    print(f\"  \ud83d\udcca Quali: MAE {quali_metrics['mae']:.3f} | Cal {quali_metrics['calibration_1sigma']:.0f}%\")\n",
        "    print(f\"  \ud83c\udfc1 Race:  MAE {race_metrics['position_mae']:.3f} | Podium {race_metrics['podium_accuracy']:.0f}% | Points {race_metrics['points_accuracy']:.0f}%\")\n",
        "    \n",
        "    # Store metrics\n",
        "    results['quali_metrics_per_race'].append(quali_metrics)\n",
        "    results['race_metrics_per_race'].append(race_metrics)\n",
        "    \n",
        "    # Update priors\n",
        "    current_priors = tuned_bayesian_update(current_priors, actual_results, season_progress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Quali summary\n",
        "quali_maes = [m['mae'] for m in results['quali_metrics_per_race'] if m['mae'] is not None]\n",
        "quali_cals = [m['calibration_1sigma'] for m in results['quali_metrics_per_race'] if m['calibration_1sigma'] is not None]\n",
        "\n",
        "print(f\"\\n\ud83c\udfce\ufe0f  QUALIFYING PREDICTIONS:\")\n",
        "print(f\"   MAE: {np.mean(quali_maes):.3f}\")\n",
        "print(f\"   Calibration (1\u03c3): {np.mean(quali_cals):.1f}%\")\n",
        "print(f\"   Races: {len(quali_maes)}\")\n",
        "\n",
        "# Race summary\n",
        "race_maes = [m['position_mae'] for m in results['race_metrics_per_race'] if m['position_mae'] is not None]\n",
        "podium_accs = [m['podium_accuracy'] for m in results['race_metrics_per_race'] if m['podium_accuracy'] is not None]\n",
        "points_accs = [m['points_accuracy'] for m in results['race_metrics_per_race'] if m['points_accuracy'] is not None]\n",
        "dnf_briers = [m['dnf_brier_score'] for m in results['race_metrics_per_race'] if m['dnf_brier_score'] is not None]\n",
        "\n",
        "print(f\"\\n\ud83c\udfc1 RACE PREDICTIONS:\")\n",
        "print(f\"   Position MAE: \u00b1{np.mean(race_maes):.1f} positions\")\n",
        "print(f\"   Podium Accuracy: {np.mean(podium_accs):.1f}%\")\n",
        "print(f\"   Points Accuracy: {np.mean(points_accs):.1f}%\")\n",
        "print(f\"   DNF Brier Score: {np.mean(dnf_briers):.3f} (lower = better)\")\n",
        "print(f\"   Races: {len(race_maes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"WHAT THIS MEANS:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\u2022 Quali predictions: {np.mean(quali_maes):.3f} MAE\")\n",
        "print(f\"\u2022 Race predictions: \u00b1{np.mean(race_maes):.1f} positions\")\n",
        "print(f\"\u2022 Podium predictions: {np.mean(podium_accs):.0f}% correct\")\n",
        "print(f\"\u2022 Points predictions: {np.mean(points_accs):.0f}% correct\")\n",
        "print(f\"\\nUsers get FULL race weekend predictions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = Path('../data/processed/testing_files/validation')\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(output_path / 'complete_system_validation.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'test_season': TEST_SEASON,\n",
        "        'quali_summary': {\n",
        "            'mae': float(np.mean(quali_maes)),\n",
        "            'calibration': float(np.mean(quali_cals))\n",
        "        },\n",
        "        'race_summary': {\n",
        "            'position_mae': float(np.mean(race_maes)),\n",
        "            'podium_accuracy': float(np.mean(podium_accs)),\n",
        "            'points_accuracy': float(np.mean(points_accs)),\n",
        "            'dnf_brier': float(np.mean(dnf_briers))\n",
        "        },\n",
        "        'per_race_metrics': {\n",
        "            'quali': results['quali_metrics_per_race'],\n",
        "            'race': results['race_metrics_per_race']\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\n\u2705 Results saved to: {output_path / 'complete_system_validation.json'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}