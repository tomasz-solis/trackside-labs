{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overtaking Difficulty - Formula Comparison Test\n",
    "\n",
    "**Goal:** Test multiple formulas and automatically pick the best one.\n",
    "\n",
    "**Baseline:** 89.7% podium accuracy (without overtaking difficulty)\n",
    "\n",
    "**Test 5 formulas:**\n",
    "1. Linear (0.5-1.5)\n",
    "2. Square Root (diminishing returns)\n",
    "3. Conservative (0.7-1.3)\n",
    "4. Aggressive (0.3-1.7)\n",
    "5. Baseline (no factor)\n",
    "\n",
    "**Winner:** Highest podium accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Run \n",
    "```python scripts/extract_overtaking_likelihood.py```\n",
    "to create \n",
    "```../data/historical/overtaking_difficulty.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import fastf1 as ff1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.getLogger('fastf1').setLevel(logging.ERROR)\n",
    "\n",
    "ff1.Cache.enable_cache('../data/raw/.fastf1_cache')\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 24 tracks\n",
      "✅ Loaded 24 drivers\n"
     ]
    }
   ],
   "source": [
    "# Load overtaking difficulty\n",
    "with open('../data/historical/overtaking_difficulty.json') as f:\n",
    "    overtaking_data = json.load(f)\n",
    "\n",
    "# Load 2024 priors\n",
    "with open('../data/processed/testing_files/2024_season_characteristics.json') as f:\n",
    "    priors_2024 = json.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(overtaking_data['tracks'])} tracks\")\n",
    "print(f\"✅ Loaded {len(priors_2024['drivers'])} drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Test Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula test (Monaco):\n",
      "  linear          → 0.60\n",
      "  sqrt            → 0.82\n",
      "  conservative    → 0.76\n",
      "  aggressive      → 0.44\n",
      "  baseline        → 1.00\n"
     ]
    }
   ],
   "source": [
    "# Define all formula variants to test\n",
    "FORMULAS = {\n",
    "    'linear': lambda d: 0.5 + d,\n",
    "    'sqrt': lambda d: 0.5 + (d ** 0.5),\n",
    "    'conservative': lambda d: 0.7 + (d * 0.6),\n",
    "    'aggressive': lambda d: 0.3 + (d * 1.4),\n",
    "    'baseline': lambda d: 1.0\n",
    "}\n",
    "\n",
    "def get_overtaking_factor(race_name, formula_key='linear'):\n",
    "    race_key = race_name.lower().replace(' ', '_')\n",
    "    if race_key in overtaking_data['tracks']:\n",
    "        difficulty = overtaking_data['tracks'][race_key]['difficulty']\n",
    "        return round(FORMULAS[formula_key](difficulty), 2)\n",
    "    return 1.0\n",
    "\n",
    "# Test formulas on Monaco\n",
    "print(\"Formula test (Monaco):\")\n",
    "for key in FORMULAS.keys():\n",
    "    factor = get_overtaking_factor('Monaco Grand Prix', key)\n",
    "    print(f\"  {key:15} → {factor:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Test configuration\n",
    "TEST_SEASON = {'from': 2024, 'to': 2025}\n",
    "TRACKED_DRIVERS = ['VER', 'HAM', 'LEC', 'NOR', 'SAI', 'PER', 'RUS', 'ALO']\n",
    "\n",
    "# Tuned parameters from 21B\n",
    "PARAMS = {\n",
    "    'racecraft_weight': 6,\n",
    "    'uncertainty_growth': 1.3,\n",
    "    'dnf_multiplier': 1.8,\n",
    "    'update_rate': 0.15\n",
    "}\n",
    "\n",
    "SPRINT_PARAMS = {\n",
    "    'uncertainty_multiplier': 1.3,\n",
    "    'racecraft_factor': 0.5,\n",
    "    'dnf_risk_factor': 0.5,\n",
    "    'quali_weight': 0.6,\n",
    "    'sprint_weight': 0.4\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_conventional_weekend(session_quali, session_race):\n",
    "    results = {'quali': {}, 'race': {}}\n",
    "    \n",
    "    for _, row in session_quali.results.iterrows():\n",
    "        driver = row['Abbreviation']\n",
    "        quali_pos = row['Position']\n",
    "        if pd.notna(driver) and pd.notna(quali_pos) and quali_pos != '':\n",
    "            try:\n",
    "                results['quali'][driver] = int(quali_pos)\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    for _, row in session_race.results.iterrows():\n",
    "        driver = row['Abbreviation']\n",
    "        race_pos = row['Position']\n",
    "        if pd.notna(driver) and driver in results['quali']:\n",
    "            try:\n",
    "                rpos = int(race_pos) if pd.notna(race_pos) and race_pos != '' else 20\n",
    "            except (ValueError, TypeError):\n",
    "                rpos = 20\n",
    "            \n",
    "            dnf = row.dnf if hasattr(row, 'dnf') else False\n",
    "            status = str(row['Status']) if 'Status' in row else ''\n",
    "            if not dnf and status:\n",
    "                dnf = 'Finished' not in status and '+' not in status\n",
    "            \n",
    "            results['race'][driver] = {'race_pos': rpos, 'dnf': dnf}\n",
    "    \n",
    "    return results if results['quali'] and results['race'] else None\n",
    "\n",
    "def extract_sprint_weekend(quali, sprint_quali, sprint_race, gp_race):\n",
    "    results = {'quali': {}, 'sprint_quali': {}, 'sprint_race': {}, 'race': {}}\n",
    "    \n",
    "    for _, row in quali.results.iterrows():\n",
    "        driver = row['Abbreviation']\n",
    "        pos = row['Position']\n",
    "        if pd.notna(driver) and pd.notna(pos) and pos != '':\n",
    "            try:\n",
    "                results['quali'][driver] = int(pos)\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    for _, row in sprint_race.results.iterrows():\n",
    "        driver = row['Abbreviation']\n",
    "        race_pos_raw = row['Position']\n",
    "        grid_pos_raw = row['GridPosition']\n",
    "        \n",
    "        dnf = row.dnf if hasattr(row, 'dnf') else False\n",
    "        status = str(row['Status']) if 'Status' in row else ''\n",
    "        if not dnf and status:\n",
    "            dnf = 'Finished' not in status and '+' not in status\n",
    "        \n",
    "        if pd.notna(driver):\n",
    "            try:\n",
    "                race_pos = int(race_pos_raw) if pd.notna(race_pos_raw) and race_pos_raw != '' else 20\n",
    "            except (ValueError, TypeError):\n",
    "                race_pos = 20\n",
    "            \n",
    "            try:\n",
    "                grid_pos = int(grid_pos_raw) if pd.notna(grid_pos_raw) and grid_pos_raw != '' else race_pos\n",
    "            except (ValueError, TypeError):\n",
    "                grid_pos = race_pos\n",
    "            \n",
    "            results['sprint_race'][driver] = {'grid_pos': grid_pos, 'race_pos': race_pos, 'dnf': dnf}\n",
    "            results['sprint_quali'][driver] = grid_pos\n",
    "    \n",
    "    for _, row in gp_race.results.iterrows():\n",
    "        driver = row['Abbreviation']\n",
    "        pos = row['Position']\n",
    "        if pd.notna(driver) and driver in results['quali']:\n",
    "            try:\n",
    "                race_pos = int(pos) if pd.notna(pos) and pos != '' else 20\n",
    "            except (ValueError, TypeError):\n",
    "                race_pos = 20\n",
    "            \n",
    "            dnf = row.dnf if hasattr(row, 'dnf') else False\n",
    "            status = str(row['Status']) if 'Status' in row else ''\n",
    "            if not dnf and status:\n",
    "                dnf = 'Finished' not in status and '+' not in status\n",
    "            \n",
    "            results['race'][driver] = {'race_pos': race_pos, 'dnf': dnf}\n",
    "    \n",
    "    return results if results['quali'] and results['race'] else None\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions (Modified for Overtaking Factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction functions defined (with overtaking factor)\n"
     ]
    }
   ],
   "source": [
    "def predict_conventional_race(quali_results, driver_priors, race_name, formula_key='linear'):\n",
    "    predictions = {}\n",
    "    overtaking_factor = get_overtaking_factor(race_name, formula_key)\n",
    "    \n",
    "    for driver, quali_pos in quali_results.items():\n",
    "        if driver not in driver_priors['drivers']:\n",
    "            continue\n",
    "        \n",
    "        driver_data = driver_priors['drivers'][driver]\n",
    "        racecraft_score = driver_data.get('racecraft', {}).get('skill_score', 0.5)\n",
    "        dnf_prob = driver_data.get('dnf_risk', {}).get('rate', 0.1)\n",
    "        quali_uncertainty = driver_data['pace']['uncertainty']\n",
    "        \n",
    "        # MODIFIED: Track-aware racecraft\n",
    "        racecraft_delta = (racecraft_score - 0.5) * PARAMS['racecraft_weight'] * overtaking_factor\n",
    "        expected_pos = quali_pos - racecraft_delta\n",
    "        expected_pos = np.clip(expected_pos, 1, 20)\n",
    "        \n",
    "        uncertainty = quali_uncertainty * PARAMS['uncertainty_growth']\n",
    "        \n",
    "        predictions[driver] = {\n",
    "            'expected_pos': expected_pos,\n",
    "            'uncertainty': uncertainty,\n",
    "            'dnf_prob': dnf_prob\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_sprint_race(sprint_quali_results, driver_priors, race_name, formula_key='linear'):\n",
    "    predictions = {}\n",
    "    overtaking_factor = get_overtaking_factor(race_name, formula_key)\n",
    "    \n",
    "    for driver, quali_pos in sprint_quali_results.items():\n",
    "        if driver not in driver_priors['drivers']:\n",
    "            continue\n",
    "        \n",
    "        driver_data = driver_priors['drivers'][driver]\n",
    "        racecraft_score = driver_data.get('racecraft', {}).get('skill_score', 0.5)\n",
    "        dnf_prob = driver_data.get('dnf_risk', {}).get('rate', 0.1)\n",
    "        quali_uncertainty = driver_data['pace']['uncertainty']\n",
    "        \n",
    "        # MODIFIED: Track-aware racecraft for sprint (shorter race)\n",
    "        racecraft_delta = (racecraft_score - 0.5) * PARAMS['racecraft_weight'] * SPRINT_PARAMS['racecraft_factor'] * overtaking_factor\n",
    "        expected_pos = quali_pos - racecraft_delta\n",
    "        expected_pos = np.clip(expected_pos, 1, 20)\n",
    "        \n",
    "        uncertainty = quali_uncertainty * SPRINT_PARAMS['uncertainty_multiplier']\n",
    "        dnf_prob_sprint = dnf_prob * SPRINT_PARAMS['dnf_risk_factor']\n",
    "        \n",
    "        predictions[driver] = {\n",
    "            'expected_pos': expected_pos,\n",
    "            'uncertainty': uncertainty,\n",
    "            'dnf_prob': dnf_prob_sprint\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_sprint_weekend_gp(friday_quali, sprint_race_result, driver_priors, race_name, formula_key='linear'):\n",
    "    predictions = {}\n",
    "    overtaking_factor = get_overtaking_factor(race_name, formula_key)\n",
    "    \n",
    "    for driver in friday_quali.keys():\n",
    "        if driver not in driver_priors['drivers'] or driver not in sprint_race_result:\n",
    "            continue\n",
    "        \n",
    "        driver_data = driver_priors['drivers'][driver]\n",
    "        sprint_data = sprint_race_result[driver]\n",
    "        \n",
    "        racecraft_score = driver_data.get('racecraft', {}).get('skill_score', 0.5)\n",
    "        dnf_prob = driver_data.get('dnf_risk', {}).get('rate', 0.1)\n",
    "        quali_uncertainty = driver_data['pace']['uncertainty']\n",
    "        \n",
    "        # MODIFIED: Track-aware racecraft\n",
    "        racecraft_delta = (racecraft_score - 0.5) * PARAMS['racecraft_weight'] * overtaking_factor\n",
    "        \n",
    "        quali_contribution = (friday_quali[driver] - racecraft_delta) * SPRINT_PARAMS['quali_weight']\n",
    "        sprint_contribution = sprint_data['race_pos'] * SPRINT_PARAMS['sprint_weight']\n",
    "        \n",
    "        expected_pos = quali_contribution + sprint_contribution\n",
    "        expected_pos = np.clip(expected_pos, 1, 20)\n",
    "        \n",
    "        uncertainty = quali_uncertainty * PARAMS['uncertainty_growth']\n",
    "        dnf_adjustment = PARAMS['dnf_multiplier'] if sprint_data['dnf'] else 1.0\n",
    "        dnf_prob_adjusted = min(dnf_prob * dnf_adjustment, 0.5)\n",
    "        \n",
    "        predictions[driver] = {\n",
    "            'expected_pos': expected_pos,\n",
    "            'uncertainty': uncertainty,\n",
    "            'dnf_prob': dnf_prob_adjusted\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"✅ Prediction functions defined (with overtaking factor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics calculation defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_race_metrics(predictions, actual_results, points_threshold=10):\n",
    "    if not predictions:\n",
    "        return {'position_mae': None, 'podium_accuracy': None, 'points_accuracy': None, 'dnf_brier': None}\n",
    "    \n",
    "    position_errors = []\n",
    "    podium_correct = []\n",
    "    points_correct = []\n",
    "    dnf_brier_scores = []\n",
    "    \n",
    "    for driver, pred in predictions.items():\n",
    "        if driver not in actual_results:\n",
    "            continue\n",
    "        \n",
    "        actual = actual_results[driver]\n",
    "        position_errors.append(abs(pred['expected_pos'] - actual['race_pos']))\n",
    "        \n",
    "        pred_podium = pred['expected_pos'] <= 3\n",
    "        actual_podium = actual['race_pos'] <= 3\n",
    "        podium_correct.append(pred_podium == actual_podium)\n",
    "        \n",
    "        pred_points = pred['expected_pos'] <= points_threshold\n",
    "        actual_points = actual['race_pos'] <= points_threshold\n",
    "        points_correct.append(pred_points == actual_points)\n",
    "        \n",
    "        actual_dnf_binary = 1 if actual['dnf'] else 0\n",
    "        brier = (pred['dnf_prob'] - actual_dnf_binary) ** 2\n",
    "        dnf_brier_scores.append(brier)\n",
    "    \n",
    "    if not position_errors:\n",
    "        return {'position_mae': None, 'podium_accuracy': None, 'points_accuracy': None, 'dnf_brier': None}\n",
    "    \n",
    "    return {\n",
    "        'position_mae': np.mean(position_errors),\n",
    "        'podium_accuracy': np.mean(podium_correct) * 100,\n",
    "        'points_accuracy': np.mean(points_correct) * 100,\n",
    "        'dnf_brier': np.mean(dnf_brier_scores),\n",
    "        'n': len(position_errors)\n",
    "    }\n",
    "\n",
    "print(\"✅ Metrics calculation defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def run_validation(formula_key='linear', verbose=False):\n",
    "    \"\"\"Run validation with specified overtaking formula.\"\"\"\n",
    "    schedule = ff1.get_event_schedule(TEST_SEASON['to'])\n",
    "    current_priors = copy.deepcopy(priors_2024)\n",
    "    \n",
    "    results = {\n",
    "        'conventional_gp': [],\n",
    "        'sprint_race': [],\n",
    "        'sprint_gp': []\n",
    "    }\n",
    "    \n",
    "    for _, event in schedule.iterrows():\n",
    "        race_name = event['EventName']\n",
    "        \n",
    "        if 'testing' in race_name.lower():\n",
    "            continue\n",
    "        \n",
    "        event_format = event['EventFormat'].lower()\n",
    "        weekend_type = 'sprint' if 'sprint' in event_format else 'conventional'\n",
    "        \n",
    "        if weekend_type not in ['conventional', 'sprint']:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            if weekend_type == 'conventional':\n",
    "                quali = ff1.get_session(TEST_SEASON['to'], race_name, 'Q')\n",
    "                quali.load(laps=False, telemetry=False, weather=False)\n",
    "                race = ff1.get_session(TEST_SEASON['to'], race_name, 'R')\n",
    "                race.load(laps=False, telemetry=False, weather=False)\n",
    "                \n",
    "                weekend_data = extract_conventional_weekend(quali, race)\n",
    "                if not weekend_data:\n",
    "                    continue\n",
    "                \n",
    "                gp_predictions = predict_conventional_race(\n",
    "                    weekend_data['quali'], \n",
    "                    current_priors, \n",
    "                    race_name,\n",
    "                    formula_key\n",
    "                )\n",
    "                gp_metrics = calculate_race_metrics(gp_predictions, weekend_data['race'])\n",
    "                if gp_metrics['position_mae'] is not None:\n",
    "                    results['conventional_gp'].append(gp_metrics)\n",
    "            \n",
    "            else:  # sprint\n",
    "                quali = ff1.get_session(TEST_SEASON['to'], race_name, 'Q')\n",
    "                quali.load(laps=False, telemetry=False, weather=False)\n",
    "                sprint_quali = ff1.get_session(TEST_SEASON['to'], race_name, 'SQ')\n",
    "                sprint_quali.load(laps=False, telemetry=False, weather=False)\n",
    "                sprint_race = ff1.get_session(TEST_SEASON['to'], race_name, 'S')\n",
    "                sprint_race.load(laps=False, telemetry=False, weather=False)\n",
    "                gp_race = ff1.get_session(TEST_SEASON['to'], race_name, 'R')\n",
    "                gp_race.load(laps=False, telemetry=False, weather=False)\n",
    "                \n",
    "                weekend_data = extract_sprint_weekend(quali, sprint_quali, sprint_race, gp_race)\n",
    "                if not weekend_data:\n",
    "                    continue\n",
    "                \n",
    "                sprint_predictions = predict_sprint_race(\n",
    "                    weekend_data['sprint_quali'],\n",
    "                    current_priors,\n",
    "                    race_name,\n",
    "                    formula_key\n",
    "                )\n",
    "                sprint_metrics = calculate_race_metrics(sprint_predictions, weekend_data['sprint_race'], 8)\n",
    "                if sprint_metrics['position_mae'] is not None:\n",
    "                    results['sprint_race'].append(sprint_metrics)\n",
    "                \n",
    "                gp_predictions = predict_sprint_weekend_gp(\n",
    "                    weekend_data['quali'],\n",
    "                    weekend_data['sprint_race'],\n",
    "                    current_priors,\n",
    "                    race_name,\n",
    "                    formula_key\n",
    "                )\n",
    "                gp_metrics = calculate_race_metrics(gp_predictions, weekend_data['race'])\n",
    "                if gp_metrics['position_mae'] is not None:\n",
    "                    results['sprint_gp'].append(gp_metrics)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error at {race_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate combined metrics\n",
    "    all_gp = results['conventional_gp'] + results['sprint_gp']\n",
    "    \n",
    "    if all_gp:\n",
    "        combined = {\n",
    "            'position_mae': np.mean([m['position_mae'] for m in all_gp]),\n",
    "            'podium_accuracy': np.mean([m['podium_accuracy'] for m in all_gp]),\n",
    "            'points_accuracy': np.mean([m['points_accuracy'] for m in all_gp]),\n",
    "            'total_races': len(all_gp)\n",
    "        }\n",
    "    else:\n",
    "        combined = {'position_mae': None, 'podium_accuracy': None, 'points_accuracy': None, 'total_races': 0}\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"✅ Validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests for All Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation for all formulas...\n",
      "This will take ~10-15 minutes\n",
      "\n",
      "[1/5] Testing linear... ✓ Podium: 89.9%, MAE: ±3.46\n",
      "[2/5] Testing sqrt... ✓ Podium: 89.7%, MAE: ±3.50\n",
      "[3/5] Testing conservative... ✓ Podium: 90.2%, MAE: ±3.46\n",
      "[4/5] Testing aggressive... ✓ Podium: 89.7%, MAE: ±3.46\n",
      "[5/5] Testing baseline... ✓ Podium: 90.4%, MAE: ±3.46\n",
      "\n",
      "✅ All tests complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running validation for all formulas...\")\n",
    "print(\"This will take ~10-15 minutes\\n\")\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for i, formula_key in enumerate(FORMULAS.keys(), 1):\n",
    "    print(f\"[{i}/5] Testing {formula_key}...\", end=' ')\n",
    "    \n",
    "    results = run_validation(formula_key, verbose=False)\n",
    "    test_results[formula_key] = results\n",
    "    \n",
    "    print(f\"✓ Podium: {results['podium_accuracy']:.1f}%, MAE: ±{results['position_mae']:.2f}\")\n",
    "\n",
    "print(\"\\n✅ All tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORMULA COMPARISON\n",
      "================================================================================\n",
      "Formula              Podium       MAE          Points       vs Baseline\n",
      "--------------------------------------------------------------------------------\n",
      "baseline                   90.4%  ±    3.46         76.0%        0.0%\n",
      "conservative               90.2%  ±    3.46         75.7%       -0.3%\n",
      "linear                     89.9%  ±    3.46         75.7%       -0.5%\n",
      "aggressive                 89.7%  ±    3.46         75.5%       -0.8%\n",
      "sqrt                       89.7%  ±    3.50         75.2%       -0.8%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"FORMULA COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Formula':<20} {'Podium':<12} {'MAE':<12} {'Points':<12} {'vs Baseline'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_podium = test_results['baseline']['podium_accuracy']\n",
    "\n",
    "for formula_key, results in sorted(test_results.items(), \n",
    "                                   key=lambda x: x[1]['podium_accuracy'], \n",
    "                                   reverse=True):\n",
    "    podium = results['podium_accuracy']\n",
    "    mae = results['position_mae']\n",
    "    points = results['points_accuracy']\n",
    "    diff = podium - baseline_podium\n",
    "    \n",
    "    diff_str = f\"+{diff:.1f}%\" if diff > 0 else f\"{diff:.1f}%\"\n",
    "    \n",
    "    print(f\"{formula_key:<20} {podium:>10.1f}%  ±{mae:>8.2f}   {points:>10.1f}%  {diff_str:>10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WINNER\n",
      "================================================================================\n",
      "\n",
      "Best formula: BASELINE\n",
      "\n",
      "Results:\n",
      "  Podium Accuracy: 90.4%\n",
      "  Position MAE: ±3.46\n",
      "  Points Accuracy: 76.0%\n",
      "\n",
      "Improvement vs baseline: +0.0%\n",
      "\n",
      "⚠️  BASELINE WON - Overtaking difficulty doesn't improve predictions!\n",
      "   → Archive the idea and move on.\n"
     ]
    }
   ],
   "source": [
    "# Find best formula (highest podium accuracy)\n",
    "winner = max(test_results.items(), key=lambda x: x[1]['podium_accuracy'])\n",
    "winner_key = winner[0]\n",
    "winner_results = winner[1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WINNER\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest formula: {winner_key.upper()}\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Podium Accuracy: {winner_results['podium_accuracy']:.1f}%\")\n",
    "print(f\"  Position MAE: ±{winner_results['position_mae']:.2f}\")\n",
    "print(f\"  Points Accuracy: {winner_results['points_accuracy']:.1f}%\")\n",
    "print(f\"\\nImprovement vs baseline: {winner_results['podium_accuracy'] - baseline_podium:+.1f}%\")\n",
    "\n",
    "if winner_key == 'baseline':\n",
    "    print(\"\\n⚠️  BASELINE WON - Overtaking difficulty doesn't improve predictions!\")\n",
    "    print(\"   → Archive the idea and move on.\")\n",
    "else:\n",
    "    print(f\"\\n✅ {winner_key.upper()} formula improves predictions!\")\n",
    "    print(f\"   → Use this formula: factor = {FORMULAS[winner_key].__code__.co_consts[1]}\")\n",
    "    print(\"   → Productionize it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../data/processed/testing_files/overtaking_formula_test.json\n"
     ]
    }
   ],
   "source": [
    "# Save test results\n",
    "output = {\n",
    "    'test_date': str(pd.Timestamp.now()),\n",
    "    'winner': winner_key,\n",
    "    'winner_results': winner_results,\n",
    "    'all_results': test_results,\n",
    "    'baseline_podium': baseline_podium\n",
    "}\n",
    "\n",
    "with open('../data/processed/testing_files/overtaking_formula_test.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"✅ Results saved to: ../data/processed/testing_files/overtaking_formula_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monaco Binary: 90.7%\n"
     ]
    }
   ],
   "source": [
    "# Test 6th formula: Monaco-only flag\n",
    "FORMULAS['monaco_binary'] = lambda d: 0.5 if d < 0.15 else 1.0\n",
    "\n",
    "# Re-run validation\n",
    "results = run_validation('monaco_binary')\n",
    "print(f\"Monaco Binary: {results['podium_accuracy']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Monaco Racecraft Factor\n",
    "\n",
    "**Date:** 2025-12-31\n",
    "**Result:** +0.3% accuracy (90.4% → 90.7%)\n",
    "\n",
    "### Decision: NOT IMPLEMENTED\n",
    "\n",
    "**Reasons:**\n",
    "1. 2026 regulation reset (smaller cars) may invalidate factor\n",
    "2. Based on outlier performances (Verstappen Brazil 2024/2025)\n",
    "3. Requires annual revalidation for minimal gain\n",
    "4. Adds technical debt for 0.3% improvement\n",
    "5. Bayesian uncertainty already captures track variability\n",
    "\n",
    "**Lesson:** \n",
    "Empirical validation is necessary but not sufficient.\n",
    "Production features must be maintainable across regulation changes.\n",
    "Small accuracy gains don't justify ongoing maintenance costs.\n",
    "\n",
    "**Status:** Tested and validated, but rejected for production use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
