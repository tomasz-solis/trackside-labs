{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Guide\n",
    "\n",
    "This notebook helps tune the model's hyperparameters using 2025 season data.\n",
    "\n",
    "**Target:** Minimize MAE (Mean Absolute Error) < 2.5 positions\n",
    "\n",
    "**Parameters to tune:**\n",
    "- Qualifying weights (model vs practice)\n",
    "- Race weights (grid, pace, tire, overtaking)\n",
    "- DNF model parameters\n",
    "- Bayesian ranking volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import yaml\n",
    "\n",
    "# Load current config\n",
    "with open('../config/default.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline: Current Performance\n",
    "\n",
    "First, establish baseline MAE with current hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation results\n",
    "validation_file = Path('../outputs/validation_results.csv')\n",
    "\n",
    "if validation_file.exists():\n",
    "    df = pd.read_csv(validation_file)\n",
    "    baseline_mae = df['mae'].mean()\n",
    "    baseline_winner_acc = df['winner_correct'].mean() * 100\n",
    "    \n",
    "    print(\"ðŸ“Š Baseline Performance (2025 season):\")\n",
    "    print(f\"   MAE: {baseline_mae:.2f} positions\")\n",
    "    print(f\"   Winner Accuracy: {baseline_winner_acc:.1f}%\")\n",
    "    print(\"   Target: MAE < 2.5\")\n",
    "else:\n",
    "    print(\"âš ï¸ Run validation_report.ipynb first to generate baseline\")\n",
    "    baseline_mae = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Race Simulation Weights\n",
    "\n",
    "Test different weight combinations for race prediction.\n",
    "\n",
    "**Current weights:**\n",
    "- Grid: 30%\n",
    "- Pace: 40%\n",
    "- Tire Deg: 15%\n",
    "- Overtaking: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight combinations to test\n",
    "weight_configs = [\n",
    "    {'grid': 0.30, 'pace': 0.40, 'tire': 0.15, 'overtaking': 0.15, 'name': 'Current'},\n",
    "    {'grid': 0.35, 'pace': 0.35, 'tire': 0.15, 'overtaking': 0.15, 'name': 'More Grid'},\n",
    "    {'grid': 0.25, 'pace': 0.45, 'tire': 0.15, 'overtaking': 0.15, 'name': 'More Pace'},\n",
    "    {'grid': 0.30, 'pace': 0.35, 'tire': 0.20, 'overtaking': 0.15, 'name': 'More Tire'},\n",
    "]\n",
    "\n",
    "print(\"Weight Combinations to Test:\")\n",
    "for w in weight_configs:\n",
    "    print(f\"  {w['name']:15s}: Grid={w['grid']:.2f}, Pace={w['pace']:.2f}, Tire={w['tire']:.2f}, OT={w['overtaking']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qualifying Blend Weights\n",
    "\n",
    "Test different blends of model vs practice data.\n",
    "\n",
    "**Hypothesis:** More practice weight = better predictions when practice data quality is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different qualifying blends\n",
    "quali_configs = [\n",
    "    {'model': 0.40, 'fp3': 0.60, 'name': 'Current'},\n",
    "    {'model': 0.30, 'fp3': 0.70, 'name': 'Trust Practice More'},\n",
    "    {'model': 0.50, 'fp3': 0.50, 'name': 'Equal Weight'},\n",
    "    {'model': 0.60, 'fp3': 0.40, 'name': 'Trust Model More'},\n",
    "]\n",
    "\n",
    "print(\"\\nQualifying Blend Configurations:\")\n",
    "for q in quali_configs:\n",
    "    print(f\"  {q['name']:25s}: Model={q['model']:.0%}, FP3={q['fp3']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DNF Model Tuning\n",
    "\n",
    "Adjust baseline DNF rates and team/driver penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnf_configs = [\n",
    "    {'base': 0.10, 'team': 0.05, 'driver': 0.03, 'name': 'Current'},\n",
    "    {'base': 0.08, 'team': 0.04, 'driver': 0.02, 'name': 'Lower Risk'},\n",
    "    {'base': 0.12, 'team': 0.06, 'driver': 0.04, 'name': 'Higher Risk'},\n",
    "]\n",
    "\n",
    "print(\"\\nDNF Model Configurations:\")\n",
    "for d in dnf_configs:\n",
    "    print(f\"  {d['name']:15s}: Base={d['base']:.0%}, Team Penalty={d['team']:.0%}, Driver Penalty={d['driver']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Experiments\n",
    "\n",
    "To test these configurations:\n",
    "\n",
    "1. Update `config/default.yaml` with new parameters\n",
    "2. Run `notebooks/validation_report.ipynb` to backtest\n",
    "3. Compare MAE to baseline\n",
    "4. Keep best-performing config\n",
    "\n",
    "**Example:**\n",
    "```yaml\n",
    "race_simulation:\n",
    "  grid_weight: 0.35        # Changed from 0.30\n",
    "  pace_weight: 0.35        # Changed from 0.40\n",
    "  tire_deg_weight: 0.15\n",
    "  overtaking_weight: 0.15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Parameter Impact\n",
    "\n",
    "Plot how different parameters affect MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Impact of grid weight on MAE\n",
    "# (Run validation with different configs and collect results)\n",
    "\n",
    "# Placeholder data\n",
    "grid_weights = [0.25, 0.30, 0.35, 0.40]\n",
    "mae_scores = [2.4, 2.28, 2.5, 2.7]  # Replace with actual results\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add MAE line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=grid_weights,\n",
    "    y=mae_scores,\n",
    "    mode='lines+markers',\n",
    "    name='MAE',\n",
    "    line=dict(width=2),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "# Add target MAE line\n",
    "fig.add_hline(\n",
    "    y=2.5,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=\"Target MAE\",\n",
    "    annotation_position=\"right\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Impact of Grid Weight on Prediction Accuracy',\n",
    "    xaxis_title='Grid Weight',\n",
    "    yaxis_title='MAE (positions)',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nâœ… Optimal grid weight: {grid_weights[np.argmin(mae_scores)]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "**When tuning hyperparameters:**\n",
    "1. Change one parameter at a time\n",
    "2. Use full 2025 season for validation (not just first 6 races)\n",
    "3. Check both MAE and winner accuracy\n",
    "4. Avoid overfitting to specific races\n",
    "5. Document why you made the change\n",
    "\n",
    "**Red flags:**\n",
    "- MAE improves but winner accuracy drops â†’ overfitting to midfield\n",
    "- Extreme parameter values (> 0.50 for any single weight)\n",
    "- Confidence scores become unrealistic (< 30% or > 95%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}