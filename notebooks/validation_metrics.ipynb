{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Metrics - MAE, RMSE, Top-N Accuracy\n",
    "\n",
    "Comprehensive validation of prediction accuracy using multiple metrics.\n",
    "\n",
    "**STATUS: Template ready - awaiting 2026 race data for validation**\n",
    "\n",
    "Once Race 3-5 complete, populate validation_data below with:\n",
    "- Predicted positions (from dashboard before race)\n",
    "- Actual results (from FastF1 after race)\n",
    "\n",
    "## Metrics Tracked:\n",
    "1. **Mean Absolute Error (MAE)** - Average position error (target: < 2.5)\n",
    "2. **Root Mean Square Error (RMSE)** - Penalizes large errors (target: < 3.5)\n",
    "3. **Top 3 Accuracy** - % correct podium predictions (target: > 60%)\n",
    "4. **Top 10 Accuracy** - % correct points finishers (target: > 70%)\n",
    "5. **Position Bucket Accuracy** - P1-5, P6-10, P11-15, P16-20 (target: > 60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import fastf1\n",
    "\n",
    "# Enable FastF1 cache\n",
    "fastf1.Cache.enable_cache('data/raw/.fastf1_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Predictions vs Actual Results\n",
    "\n",
    "**TODO:** Replace placeholder data with actual predictions from 2026 races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER: To be filled with actual 2026 validation data\n",
    "# Format: {'race': 'Race Name', 'driver': 'DRV', 'predicted': int, 'actual': int}\n",
    "validation_data = [\n",
    "    # Example structure - replace with real data:\n",
    "    # {'race': 'Bahrain Grand Prix', 'driver': 'VER', 'predicted': 1, 'actual': 1},\n",
    "    # {'race': 'Bahrain Grand Prix', 'driver': 'NOR', 'predicted': 2, 'actual': 3},\n",
    "    # ...\n",
    "]\n",
    "\n",
    "if not validation_data:\n",
    "    print(\"‚ö†Ô∏è No validation data loaded. Run predictions for 2026 races first.\")\n",
    "    print(\"\\nTo collect validation data:\")\n",
    "    print(\"1. Generate predictions before each race via dashboard\")\n",
    "    print(\"2. Save predicted positions\")\n",
    "    print(\"3. After race, fetch actual results via FastF1\")\n",
    "    print(\"4. Populate validation_data list above\")\n",
    "else:\n",
    "    df = pd.DataFrame(validation_data)\n",
    "    print(f\"‚úÖ Loaded {len(df)} predictions across {df['race'].nunique()} races\")\n",
    "    print(f\"\\nRaces included: {', '.join(df['race'].unique())}\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1: Mean Absolute Error (MAE)\n",
    "\n",
    "Average absolute difference between predicted and actual positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_data:\n",
    "    mae = mean_absolute_error(df['actual'], df['predicted'])\n",
    "    print(f\"Mean Absolute Error: {mae:.2f} positions\")\n",
    "    print(f\"Target: < 2.5 positions\")\n",
    "    print(f\"Status: {'‚úÖ PASS' if mae < 2.5 else '‚ùå FAIL'}\")\n",
    "\n",
    "    # MAE by race\n",
    "    mae_by_race = df.groupby('race').apply(lambda x: mean_absolute_error(x['actual'], x['predicted']))\n",
    "    print(\"\\nMAE by Race:\")\n",
    "    print(mae_by_race.sort_values())\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    mae_by_race.plot(kind='bar', color='steelblue')\n",
    "    plt.axhline(mae, color='r', linestyle='--', label=f'Overall MAE: {mae:.2f}')\n",
    "    plt.axhline(2.5, color='g', linestyle='--', alpha=0.5, label='Target: 2.5')\n",
    "    plt.title('Mean Absolute Error by Race')\n",
    "    plt.ylabel('MAE (positions)')\n",
    "    plt.xlabel('Race')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Awaiting validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2: Root Mean Square Error (RMSE)\n",
    "\n",
    "Penalizes large prediction errors more heavily than MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_data:\n",
    "    rmse = np.sqrt(mean_squared_error(df['actual'], df['predicted']))\n",
    "    print(f\"Root Mean Square Error: {rmse:.2f} positions\")\n",
    "    print(f\"Target: < 3.5 positions\")\n",
    "    print(f\"Status: {'‚úÖ PASS' if rmse < 3.5 else '‚ùå FAIL'}\")\n",
    "    print(f\"\\nRMSE/MAE ratio: {rmse/mae:.2f}x\")\n",
    "    print(\"(Higher ratio indicates more large errors)\")\n",
    "\n",
    "    # Error distribution\n",
    "    df['error'] = df['predicted'] - df['actual']\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(df['error'], bins=range(-10, 11), edgecolor='black', alpha=0.7, color='coral')\n",
    "    plt.axvline(0, color='r', linestyle='--', linewidth=2, label='Perfect prediction')\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.xlabel('Error (predicted - actual positions)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nError statistics:\")\n",
    "    print(f\"  Mean error (bias): {df['error'].mean():.2f}\")\n",
    "    print(f\"  Std dev: {df['error'].std():.2f}\")\n",
    "    print(f\"  Median error: {df['error'].median():.1f}\")\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Awaiting validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 3: Top-N Accuracy\n",
    "\n",
    "Percentage of drivers correctly predicted in top N positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_data:\n",
    "    def top_n_accuracy(df, n):\n",
    "        \"\"\"Calculate % of drivers predicted in top N who actually finished in top N.\"\"\"\n",
    "        predicted_top_n = df[df['predicted'] <= n]['driver'].values\n",
    "        actual_top_n = df[df['actual'] <= n]['driver'].values\n",
    "        correct = len(set(predicted_top_n) & set(actual_top_n))\n",
    "        return correct / n * 100\n",
    "\n",
    "    # Calculate for different N\n",
    "    top_n_results = {}\n",
    "    targets = {'Top 1': 40, 'Top 3': 60, 'Top 5': 65, 'Top 10': 70}\n",
    "    \n",
    "    for n in [1, 3, 5, 10]:\n",
    "        accuracy = df.groupby('race').apply(lambda x: top_n_accuracy(x, n)).mean()\n",
    "        top_n_results[f'Top {n}'] = accuracy\n",
    "        target = targets[f'Top {n}']\n",
    "        status = '‚úÖ' if accuracy >= target else '‚ùå'\n",
    "        print(f\"Top {n} Accuracy: {accuracy:.1f}% (target: >{target}%) {status}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(top_n_results.keys(), top_n_results.values(), color='mediumseagreen')\n",
    "    plt.title('Top-N Prediction Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Prediction Category')\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    for i, (k, v) in enumerate(top_n_results.items()):\n",
    "        plt.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Awaiting validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 4: Position Bucket Accuracy\n",
    "\n",
    "How often drivers finish in the predicted position bucket (e.g., P1-5, P6-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_data:\n",
    "    def get_bucket(position):\n",
    "        \"\"\"Assign position to bucket.\"\"\"\n",
    "        if position <= 5:\n",
    "            return 'P1-5'\n",
    "        elif position <= 10:\n",
    "            return 'P6-10'\n",
    "        elif position <= 15:\n",
    "            return 'P11-15'\n",
    "        else:\n",
    "            return 'P16-20'\n",
    "\n",
    "    df['predicted_bucket'] = df['predicted'].apply(get_bucket)\n",
    "    df['actual_bucket'] = df['actual'].apply(get_bucket)\n",
    "    df['bucket_correct'] = df['predicted_bucket'] == df['actual_bucket']\n",
    "\n",
    "    bucket_accuracy = df.groupby('actual_bucket')['bucket_correct'].mean() * 100\n",
    "    print(\"Position Bucket Accuracy:\")\n",
    "    print(bucket_accuracy)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bucket_accuracy.plot(kind='bar', color='slateblue')\n",
    "    plt.title('Position Bucket Prediction Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Position Bucket')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.axhline(60, color='g', linestyle='--', alpha=0.5, label='Target: 60%')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    overall_bucket_acc = df['bucket_correct'].mean() * 100\n",
    "    print(f\"\\nOverall bucket accuracy: {overall_bucket_acc:.1f}%\")\n",
    "    print(f\"Target: > 60%\")\n",
    "    print(f\"Status: {'‚úÖ PASS' if overall_bucket_acc > 60 else '‚ùå FAIL'}\")\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Awaiting validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "Consolidated view of all validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_data:\n",
    "    def get_status(value, target, comparison='<'):\n",
    "        if comparison == '<':\n",
    "            return '‚úÖ PASS' if value < target else '‚ùå FAIL'\n",
    "        else:\n",
    "            return '‚úÖ PASS' if value >= target else '‚ùå FAIL'\n",
    "    \n",
    "    summary = {\n",
    "        'Metric': ['MAE', 'RMSE', 'Top 1 Accuracy', 'Top 3 Accuracy', 'Top 10 Accuracy', 'Bucket Accuracy'],\n",
    "        'Value': [\n",
    "            f\"{mae:.2f} pos\", \n",
    "            f\"{rmse:.2f} pos\", \n",
    "            f\"{top_n_results['Top 1']:.1f}%\", \n",
    "            f\"{top_n_results['Top 3']:.1f}%\", \n",
    "            f\"{top_n_results['Top 10']:.1f}%\", \n",
    "            f\"{overall_bucket_acc:.1f}%\"\n",
    "        ],\n",
    "        'Target': ['< 2.5', '< 3.5', '>= 40%', '>= 60%', '>= 70%', '> 60%'],\n",
    "        'Status': [\n",
    "            get_status(mae, 2.5, '<'),\n",
    "            get_status(rmse, 3.5, '<'),\n",
    "            get_status(top_n_results['Top 1'], 40, '>='),\n",
    "            get_status(top_n_results['Top 3'], 60, '>='),\n",
    "            get_status(top_n_results['Top 10'], 70, '>='),\n",
    "            get_status(overall_bucket_acc, 60, '>=')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION METRICS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pass_count = sum(1 for s in summary['Status'] if '‚úÖ' in s)\n",
    "    total_count = len(summary['Status'])\n",
    "    print(f\"\\n Overall: {pass_count}/{total_count} metrics passing\")\n",
    "    \n",
    "    if pass_count == total_count:\n",
    "        print(\"\\nüéâ ALL METRICS PASSING - System validated!\")\n",
    "    elif pass_count >= total_count * 0.75:\n",
    "        print(\"\\n‚úÖ Most metrics passing - System performing well\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Multiple metrics failing - Review model parameters\")\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Awaiting validation data from 2026 races\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Run predictions for Races 1-3\")\n",
    "    print(\"2. Collect actual results\")\n",
    "    print(\"3. Populate validation_data above\")\n",
    "    print(\"4. Re-run this notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
