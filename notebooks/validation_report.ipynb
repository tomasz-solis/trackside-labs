{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1 2026 Prediction Model - Validation Report\n",
    "\n",
    "Backtest analysis of 2025 season predictions to measure model accuracy and calibration.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Mean Absolute Error (MAE) for position predictions\n",
    "- Calibration curves (are 70% predictions right 70% of the time?)\n",
    "- Top-3 accuracy\n",
    "- Winner prediction rate\n",
    "\n",
    "**Note:** 2025 season complete. 2026 pre-season testing starts February 2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import fastf1 as ff1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.models.bayesian import BayesianDriverRanking\n",
    "from src.models.priors_factory import PriorsFactory\n",
    "from src.predictors.qualifying import QualifyingPredictor\n",
    "from src.predictors.race import RacePredictor\n",
    "from src.utils.performance_tracker import PerformanceTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Get 2025 season results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2025 schedule\n",
    "ff1.Cache.enable_cache('~/.fastf1_cache')\n",
    "schedule_2025 = ff1.get_event_schedule(2025)\n",
    "\n",
    "# Filter to completed races (exclude testing)\n",
    "races = schedule_2025[\n",
    "    (schedule_2025['EventFormat'] != 'testing') & \n",
    "    (schedule_2025['EventName'].notna())\n",
    "].head(10)  # First 10 races for validation\n",
    "\n",
    "print(f\"Validating against {len(races)} races from 2025 season\")\n",
    "races[['RoundNumber', 'EventName', 'Country']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Initialization\n",
    "\n",
    "Set up prediction system with 2025 baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "factory = PriorsFactory()\n",
    "priors = factory.create_priors()\n",
    "ranker = BayesianDriverRanking(priors)\n",
    "tracker = PerformanceTracker()\n",
    "\n",
    "quali_predictor = QualifyingPredictor(\n",
    "    driver_ranker=ranker,\n",
    "    performance_tracker=tracker\n",
    ")\n",
    "\n",
    "race_predictor = RacePredictor(\n",
    "    year=2025,\n",
    "    driver_chars=factory.drivers,\n",
    "    driver_chars_path=factory.driver_file,\n",
    "    performance_tracker=tracker\n",
    ")\n",
    "\n",
    "print(\"Model initialized with 2025 baseline characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Predictions vs Actuals\n",
    "\n",
    "For each race:\n",
    "1. Predict qualifying and race\n",
    "2. Compare with actual results\n",
    "3. Update Bayesian beliefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for _, event in races.iterrows():\n",
    "    race_name = event['EventName']\n",
    "    round_num = event['RoundNumber']\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Round {round_num}: {race_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Get actual results\n",
    "        session = ff1.get_session(2025, race_name, 'R')\n",
    "        session.load(laps=False, telemetry=False)\n",
    "        \n",
    "        actual_results = session.results[['Abbreviation', 'Position']].dropna()\n",
    "        actual_dict = dict(zip(actual_results['Abbreviation'], actual_results['Position'], strict=False))\n",
    "        \n",
    "        # Get qualifying grid\n",
    "        quali_session = ff1.get_session(2025, race_name, 'Q')\n",
    "        quali_session.load(laps=False, telemetry=False)\n",
    "        quali_results = quali_session.results[['Abbreviation', 'Position', 'TeamName']].dropna()\n",
    "        \n",
    "        grid = [\n",
    "            {'driver': row['Abbreviation'], 'team': row['TeamName'], 'position': row['Position']}\n",
    "            for _, row in quali_results.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Predict race\n",
    "        prediction = race_predictor.predict(\n",
    "            year=2025,\n",
    "            race_name=race_name,\n",
    "            qualifying_grid=grid,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Calculate errors\n",
    "        for pred in prediction['finish_order']:\n",
    "            driver = pred['driver']\n",
    "            if driver in actual_dict:\n",
    "                results.append({\n",
    "                    'race': race_name,\n",
    "                    'round': round_num,\n",
    "                    'driver': driver,\n",
    "                    'predicted_pos': pred['position'],\n",
    "                    'actual_pos': actual_dict[driver],\n",
    "                    'error': abs(pred['position'] - actual_dict[driver]),\n",
    "                    'confidence': pred['confidence'],\n",
    "                    'podium_prob': pred['podium_probability'],\n",
    "                    'actual_podium': 1 if actual_dict[driver] <= 3 else 0\n",
    "                })\n",
    "        \n",
    "        # Show summary\n",
    "        winner_pred = prediction['finish_order'][0]['driver']\n",
    "        winner_actual = actual_results.iloc[0]['Abbreviation']\n",
    "        mae = np.mean([r['error'] for r in results if r['race'] == race_name])\n",
    "        \n",
    "        print(f\"Winner: {winner_pred} (predicted) vs {winner_actual} (actual)\")\n",
    "        print(f\"MAE: {mae:.2f} positions\")\n",
    "        \n",
    "        # Update Bayesian beliefs\n",
    "        observations = {row['DriverNumber']: row['Position'] \n",
    "                       for _, row in session.results.dropna(subset=['Position']).iterrows()}\n",
    "        ranker.update(observations, race_name, confidence=1.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {race_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\nCollected {len(df_results)} predictions across {df_results['race'].nunique()} races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION METRICS - 2025 SEASON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mae = df_results['error'].mean()\n",
    "print(f\"\\nMean Absolute Error: {mae:.2f} positions\")\n",
    "\n",
    "median_error = df_results['error'].median()\n",
    "print(f\"Median Error: {median_error:.2f} positions\")\n",
    "\n",
    "winner_correct = sum(1 for _, g in df_results.groupby('race') \n",
    "                     if g[g['predicted_pos'] == 1].iloc[0]['actual_pos'] == 1)\n",
    "winner_accuracy = winner_correct / df_results['race'].nunique() * 100\n",
    "print(f\"\\nWinner Prediction Accuracy: {winner_accuracy:.1f}%\")\n",
    "\n",
    "top3_pred = df_results[df_results['predicted_pos'] <= 3]\n",
    "top3_accuracy = (top3_pred['actual_pos'] <= 3).mean() * 100\n",
    "print(f\"Top-3 Accuracy: {top3_accuracy:.1f}%\")\n",
    "\n",
    "perfect = (df_results['error'] == 0).mean() * 100\n",
    "print(f\"\\nPerfect Predictions: {perfect:.1f}%\")\n",
    "\n",
    "within_2 = (df_results['error'] <= 2).mean() * 100\n",
    "print(f\"Within 2 Positions: {within_2:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Distribution of Prediction Errors', 'Error by Position Group')\n",
    ")\n",
    "\n",
    "# Histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df_results['error'], nbinsx=20, name='Error Distribution'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_vline(x=mae, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=f\"MAE = {mae:.2f}\", row=1, col=1)\n",
    "\n",
    "# Box plot by position\n",
    "df_results['position_bin'] = pd.cut(df_results['predicted_pos'], \n",
    "                                     bins=[0, 5, 10, 20], \n",
    "                                     labels=['Top 5', 'P6-10', 'P11-20'])\n",
    "\n",
    "for group in ['Top 5', 'P6-10', 'P11-20']:\n",
    "    data = df_results[df_results['position_bin'] == group]['error']\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data, name=group),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"Error (positions)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Error (positions)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calibration Analysis\n",
    "\n",
    "Are our confidence levels meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin by confidence level\n",
    "df_results['confidence_bin'] = pd.cut(\n",
    "    df_results['confidence'],\n",
    "    bins=[0, 50, 65, 80, 100],\n",
    "    labels=['Low (0-50%)', 'Medium (50-65%)', 'High (65-80%)', 'Very High (80-100%)']\n",
    ")\n",
    "\n",
    "# Calculate accuracy in each bin\n",
    "calibration = df_results.groupby('confidence_bin').agg({\n",
    "    'error': ['mean', 'count'],\n",
    "    'predicted_pos': lambda x: (df_results.loc[x.index, 'error'] <= 2).mean() * 100\n",
    "}).round(2)\n",
    "\n",
    "calibration.columns = ['MAE', 'Count', 'Within 2 Pos (%)']\n",
    "print(\"\\nCalibration Table:\")\n",
    "print(calibration)\n",
    "\n",
    "# Visualization\n",
    "fig = px.bar(\n",
    "    calibration.reset_index(),\n",
    "    x='confidence_bin',\n",
    "    y='Within 2 Pos (%)',\n",
    "    title='Model Calibration: Confidence vs Accuracy',\n",
    "    labels={'confidence_bin': 'Confidence Level', 'Within 2 Pos (%)': 'Accuracy Within 2 Positions (%)'},\n",
    "    color='Within 2 Pos (%)',\n",
    "    color_continuous_scale='RdYlGn'\n",
    ")\n",
    "fig.add_hline(y=70, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"Target: 70%\")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Podium Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High probability podium predictions\n",
    "high_prob_podium = df_results[df_results['podium_prob'] > 50]\n",
    "\n",
    "if len(high_prob_podium) > 0:\n",
    "    podium_accuracy = high_prob_podium['actual_podium'].mean() * 100\n",
    "    print(\"When we predicted >50% podium chance:\")\n",
    "    print(f\"  - Number of predictions: {len(high_prob_podium)}\")\n",
    "    print(f\"  - Actual podium rate: {podium_accuracy:.1f}%\")\n",
    "\n",
    "# Calibration curve\n",
    "prob_thresholds = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "podium_rates = []\n",
    "\n",
    "for thresh in prob_thresholds:\n",
    "    subset = df_results[df_results['podium_prob'] >= thresh]\n",
    "    if len(subset) > 0:\n",
    "        rate = subset['actual_podium'].mean() * 100\n",
    "        podium_rates.append(rate)\n",
    "    else:\n",
    "        podium_rates.append(None)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=prob_thresholds,\n",
    "    y=podium_rates,\n",
    "    mode='lines+markers',\n",
    "    name='Actual',\n",
    "    line=dict(width=3)\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 100],\n",
    "    y=[0, 100],\n",
    "    mode='lines',\n",
    "    name='Perfect calibration',\n",
    "    line=dict(dash='dash', color='red')\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Podium Prediction Calibration',\n",
    "    xaxis_title='Predicted Podium Probability Threshold (%)',\n",
    "    yaxis_title='Actual Podium Rate (%)',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Race-by-Race Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_summary = df_results.groupby(['race', 'round']).agg({\n",
    "    'error': 'mean',\n",
    "    'driver': 'count'\n",
    "}).rename(columns={'error': 'MAE', 'driver': 'Predictions'}).sort_values('round')\n",
    "\n",
    "print(\"\\nRace-by-Race MAE:\")\n",
    "print(race_summary)\n",
    "\n",
    "fig = px.line(\n",
    "    race_summary.reset_index(),\n",
    "    x='round',\n",
    "    y='MAE',\n",
    "    title='Prediction Accuracy Over 2025 Season',\n",
    "    labels={'round': 'Round', 'MAE': 'Mean Absolute Error'},\n",
    "    markers=True\n",
    ")\n",
    "fig.add_hline(y=mae, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=f\"Overall MAE = {mae:.2f}\")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Driver-Specific Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_errors = df_results.groupby('driver').agg({\n",
    "    'error': ['mean', 'count']\n",
    "}).round(2)\n",
    "driver_errors.columns = ['MAE', 'Predictions']\n",
    "driver_errors = driver_errors[driver_errors['Predictions'] >= 5].sort_values('MAE')\n",
    "\n",
    "print(\"\\nDriver Prediction Accuracy (min 5 predictions):\")\n",
    "print(\"\\nMost Predictable:\")\n",
    "print(driver_errors.head(5))\n",
    "print(\"\\nLeast Predictable:\")\n",
    "print(driver_errors.tail(5))\n",
    "\n",
    "# Visualization\n",
    "fig = px.bar(\n",
    "    driver_errors.reset_index().head(10),\n",
    "    x='driver',\n",
    "    y='MAE',\n",
    "    title='Top 10 Most Predictable Drivers (2025)',\n",
    "    labels={'driver': 'Driver', 'MAE': 'Mean Absolute Error'},\n",
    "    color='MAE',\n",
    "    color_continuous_scale='RdYlGn_r'\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "bins = [0, 3, 10, 20]\n",
    "labels = ['Podium', 'Points', 'No Points']\n",
    "\n",
    "df_results['pred_bin'] = pd.cut(df_results['predicted_pos'], bins=bins, labels=labels)\n",
    "df_results['actual_bin'] = pd.cut(df_results['actual_pos'], bins=bins, labels=labels)\n",
    "\n",
    "cm = confusion_matrix(df_results['actual_bin'], df_results['pred_bin'], labels=labels)\n",
    "\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    title='Confusion Matrix: Position Categories',\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model Performance on 2025 Season:**\n",
    "\n",
    "The validation shows:\n",
    "- MAE and accuracy metrics quantify prediction quality\n",
    "- Calibration analysis reveals if confidence scores are meaningful\n",
    "- Position-specific and driver-specific errors identify where the model struggles\n",
    "- Bayesian learning improves predictions as the season progresses\n",
    "\n",
    "**Next Steps for 2026:**\n",
    "1. Update driver characteristics with 2025 season data\n",
    "2. Adjust for new team lineups (Cadillac entry, driver changes)\n",
    "3. Monitor pre-season testing (February 2026) for regulation impact\n",
    "4. Re-calibrate uncertainty given regulation reset\n",
    "5. Test predictions during first few races of 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
