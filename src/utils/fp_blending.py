"""Extract and blend FP session performance with model predictions."""

import logging
import time
from collections.abc import Callable
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

import fastf1 as ff1
import numpy as np
import pandas as pd

from src.utils import config_loader
from src.utils.team_mapping import map_team_to_characteristics

logging.getLogger("fastf1").setLevel(logging.CRITICAL)
logger = logging.getLogger(__name__)


class CircuitBreaker:
    """Circuit breaker for FastF1 API rate limiting protection."""

    def __init__(self, failure_threshold: int = 5, recovery_timeout: float = 60.0):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time: float | None = None
        self.state = "closed"  # closed, open, half_open

    def reset(self) -> None:
        """Reset circuit breaker to closed state (for testing)."""
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"

    def call(self, fn: Callable[[], Any]) -> Any:
        """Execute function with circuit breaker protection."""
        if self.state == "open":
            if (
                self.last_failure_time
                and (time.time() - self.last_failure_time) > self.recovery_timeout
            ):
                logger.info("Circuit breaker transitioning to half-open state")
                self.state = "half_open"
            else:
                raise RuntimeError(
                    "Circuit breaker is open; FastF1 requests are temporarily blocked"
                )

        try:
            result = fn()
            if self.state == "half_open":
                logger.info("Circuit breaker recovered, transitioning to closed state")
                self.failure_count = 0
                self.state = "closed"
            return result
        except Exception:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                logger.error(f"Circuit breaker opened after {self.failure_count} failures")
                self.state = "open"
            raise


# Global circuit breaker instance (shared across all FastF1 calls)
_circuit_breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=60.0)


def _fastf1_with_retry(
    fn: Callable[[], Any], max_retries: int = 3, initial_delay: float = 1.0
) -> Any:
    """Execute FastF1 API call with exponential backoff retry logic and circuit breaker protection."""
    for attempt in range(max_retries):
        try:
            return _circuit_breaker.call(fn)
        except RuntimeError as e:
            if "Circuit breaker is open" in str(e):
                raise
            raise
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            delay = initial_delay * (2**attempt)
            logger.warning(
                f"FastF1 API error (attempt {attempt + 1}/{max_retries}): {e.__class__.__name__}: {e}. Retrying in {delay:.1f}s..."
            )
            time.sleep(delay)


class FPDataError(Enum):
    """Error codes for FP data extraction failures."""

    NOT_COMPLETED = "not_completed"
    API_FAILURE = "api_failure"
    STALE_DATA = "stale_data"
    INSUFFICIENT_LAPS = "insufficient_laps"


def _extract_short_run_lap_time(valid_laps: pd.DataFrame) -> float | None:
    """Return a representative short-stint lap time (seconds) for a driver."""
    laps = valid_laps.copy()

    # Exclude in/out laps when available; short-run pace should reflect push laps.
    for pit_col in ("PitOutTime", "PitInTime"):
        if pit_col in laps.columns:
            laps = laps[laps[pit_col].isna()]

    if laps.empty:
        return None

    short_laps = pd.DataFrame()
    if "TyreLife" in laps.columns:
        tyre_life = pd.to_numeric(laps["TyreLife"], errors="coerce")
        short_laps = laps[(tyre_life >= 1) & (tyre_life <= 5)]

    candidates = short_laps if not short_laps.empty else laps
    lap_seconds = candidates["LapTime"].dt.total_seconds().dropna()
    if lap_seconds.empty:
        return None

    top_n = max(1, min(3, len(lap_seconds)))
    return float(lap_seconds.nsmallest(top_n).median())


def _extract_long_run_lap_time(valid_laps: pd.DataFrame, min_long_run_laps: int) -> float | None:
    """Return a representative long-stint lap time (seconds) for a driver."""
    laps = valid_laps.copy()
    if laps.empty:
        return None

    if "Stint" in laps.columns and laps["Stint"].notna().any():
        group_cols = ["Stint"]
        if "Compound" in laps.columns:
            group_cols.append("Compound")
        stints = [stint for _key, stint in laps.groupby(group_cols)]
    else:
        # Fallback when explicit stint numbering is unavailable.
        if "Compound" in laps.columns:
            laps = laps.sort_index()
            stint_ids = (laps["Compound"] != laps["Compound"].shift()).cumsum()
            stints = [stint for _key, stint in laps.groupby(stint_ids)]
        else:
            stints = [laps]

    best_time = None
    best_len = 0

    for stint in stints:
        if len(stint) < min_long_run_laps:
            continue

        lap_seconds = stint["LapTime"].dt.total_seconds().dropna().sort_values()
        if len(lap_seconds) < min_long_run_laps:
            continue

        # Trim one lap from each end to reduce out/in-lap contamination.
        if len(lap_seconds) > 4:
            lap_seconds = lap_seconds.iloc[1:-1]

        rep_time = float(lap_seconds.mean())

        if len(stint) > best_len:
            best_len = len(stint)
            best_time = rep_time

    return best_time


def _extract_representative_lap_time(
    valid_laps: pd.DataFrame,
    run_focus: str,
    min_long_run_laps: int,
) -> float | None:
    """Extract representative lap time for short-run or long-run focus."""
    if run_focus == "long":
        return _extract_long_run_lap_time(valid_laps, min_long_run_laps=min_long_run_laps)
    return _extract_short_run_lap_time(valid_laps)


def get_fp_team_performance(
    year: int,
    race_name: str,
    session_type: str,
    max_data_age_hours: float | None = None,
    run_focus: str = "short",
) -> tuple[dict[str, float] | None, pd.DataFrame | None, FPDataError | None]:
    """Extract team performance from practice session with staleness and lap count validation."""
    if run_focus not in {"short", "long"}:
        raise ValueError("run_focus must be one of: 'short', 'long'")

    try:
        session = _fastf1_with_retry(lambda: ff1.get_session(year, race_name, session_type))
        if session is None:
            return None, None, FPDataError.API_FAILURE

        load_result = _fastf1_with_retry(
            lambda: session.load(laps=True, telemetry=False, weather=False)
        )
        if load_result is None:
            logger.warning(f"session.load() returned None for {session_type} at {race_name}")
            return None, None, FPDataError.API_FAILURE

        if not hasattr(session, "laps") or session.laps is None or session.laps.empty:
            return None, None, FPDataError.NOT_COMPLETED

        # Enforce data freshness via config (default one week).
        if max_data_age_hours is None:
            max_data_age_hours = float(
                config_loader.get("baseline_predictor.qualifying.max_session_age_hours", 168.0)
            )

        if hasattr(session, "date") and session.date:
            session_age = datetime.now(tz=session.date.tzinfo) - session.date
            if session_age > timedelta(hours=max_data_age_hours):
                logger.warning(
                    f"{session_type} for {race_name} is "
                    f"{session_age.total_seconds() / 3600:.1f}h old "
                    f"(max {max_data_age_hours:.1f}h) - rejecting stale data"
                )
                return None, None, FPDataError.STALE_DATA

        laps = session.laps
        min_long_run_laps = int(
            config_loader.get("baseline_predictor.race.weekend_long_run_min_laps", 8)
        )

        # Reject red-flagged/truncated sessions (<10 total laps)
        if len(laps) < 10:
            logger.warning(
                f"{session_type} for {race_name} has only {len(laps)} laps - likely red-flagged, rejecting"
            )
            return None, None, FPDataError.INSUFFICIENT_LAPS

        # Build representative per-driver pace signal for requested run focus.
        best_times = []

        for driver in laps["Driver"].unique():
            driver_laps = laps[laps["Driver"] == driver]

            # Filter valid laps
            valid_laps = driver_laps[
                (driver_laps["LapTime"].notna())
                & (driver_laps["Compound"].notna())  # On tire compound
            ]

            if len(valid_laps) == 0:
                continue

            representative_time = _extract_representative_lap_time(
                valid_laps,
                run_focus=run_focus,
                min_long_run_laps=min_long_run_laps,
            )
            if representative_time is None:
                continue

            team_raw = driver_laps["Team"].iloc[0]
            if pd.isna(team_raw):
                continue
            team = map_team_to_characteristics(team_raw) or str(team_raw)

            best_times.append({"driver": driver, "team": team, "time": representative_time})

        if not best_times:
            return None, None, FPDataError.INSUFFICIENT_LAPS

        # Get median time per team (robust to one driver having issues)
        team_times = {}

        for entry in best_times:
            team = entry["team"]
            if team not in team_times:
                team_times[team] = []
            team_times[team].append(entry["time"])

        team_medians = {team: np.median(times) for team, times in team_times.items()}

        # Convert to relative performance (0-1 scale)
        # Invert: Faster time = Higher score
        fastest = min(team_medians.values())
        slowest = max(team_medians.values())

        if fastest == slowest:
            return {team: 0.5 for team in team_medians}, laps, None

        team_performance = {
            team: 1.0 - (time - fastest) / (slowest - fastest)
            for team, time in team_medians.items()
        }

        return team_performance, laps, None

    except Exception as e:
        logger.warning(
            f"FastF1 API failure for {session_type} at {race_name} ({year}): {e.__class__.__name__}: {e}"
        )
        return None, None, FPDataError.API_FAILURE


def get_best_fp_performance(
    year: int,
    race_name: str,
    is_sprint: bool = False,
    qualifying_stage: str = "auto",
) -> tuple[str | None, dict[str, float] | None, pd.DataFrame | None]:
    """Get best available practice session with staleness checks and error reporting."""
    stage = (qualifying_stage or "auto").strip().lower()
    if stage not in {"auto", "sprint", "main"}:
        raise ValueError("qualifying_stage must be one of: 'auto', 'sprint', 'main'")

    session_priority: list[tuple[str, str, float]]
    if is_sprint:
        if stage == "sprint":
            # Sprint Qualifying prediction should be anchored to pre-SQ context.
            session_priority = [("FP1", "FP1 short-stint", 1.0)]
        else:
            # Main qualifying: blend all available short-run signals.
            session_priority = [
                ("Sprint Qualifying", "Sprint Qualifying short-stint", 1.00),
                ("Sprint", "Sprint pace signal", 0.55),
                ("FP1", "FP1 short-stint", 0.70),
            ]
    else:
        # Normal weekend: blend short-stint signals instead of hard FP3 fallback.
        session_priority = [
            ("FP3", "FP3 short-stint", 1.00),
            ("FP2", "FP2 short-stint", 0.82),
            ("FP1", "FP1 short-stint", 0.68),
        ]

    errors_encountered = []
    available_sessions: list[dict[str, Any]] = []
    for session_code, session_label, session_weight in session_priority:
        fp_data, session_laps, error = get_fp_team_performance(year, race_name, session_code)
        if fp_data is not None:
            available_sessions.append(
                {
                    "code": session_code,
                    "label": session_label,
                    "weight": float(session_weight),
                    "data": fp_data,
                    "laps": session_laps,
                }
            )
            continue
        if error:
            errors_encountered.append((session_code, error))

    if available_sessions:
        if len(available_sessions) == 1:
            selected = available_sessions[0]
            logger.info(f"Using {selected['label']} for blending")
            return selected["label"], selected["data"], selected["laps"]

        weighted_totals: dict[str, float] = {}
        weighted_counts: dict[str, float] = {}
        for session_info in available_sessions:
            weight = float(session_info["weight"])
            for team, score in session_info["data"].items():
                weighted_totals[team] = weighted_totals.get(team, 0.0) + (score * weight)
                weighted_counts[team] = weighted_counts.get(team, 0.0) + weight

        blended = {
            team: weighted_totals[team] / weighted_counts[team]
            for team in weighted_totals
            if weighted_counts.get(team, 0.0) > 0.0
        }
        if blended:
            included = " + ".join([item["code"] for item in available_sessions])
            primary = max(available_sessions, key=lambda item: item["weight"])
            session_label = f"Short-stint blend ({included})"
            logger.info(f"Using {session_label} for blending")
            return session_label, blended, primary["laps"]

    # Log why we're falling back to model-only
    if errors_encountered:
        error_summary = ", ".join([f"{s}: {e.value}" for s, e in errors_encountered])
        logger.info(f"No valid practice data ({error_summary}) - using model-only predictions")
    else:
        logger.info("No practice data available - using model-only predictions")

    return None, None, None


def blend_team_strength(
    model_strength: dict[str, float],
    fp_performance: dict[str, float] | None,
    blend_weight: float = 0.7,
) -> dict[str, float]:
    """Blend model predictions with FP data (70% practice + 30% model)."""
    if fp_performance is None:
        return model_strength

    # Validate team name matches
    model_teams = set(model_strength.keys())
    fp_teams = set(fp_performance.keys())

    missing_from_fp = model_teams - fp_teams
    extra_in_fp = fp_teams - model_teams

    if missing_from_fp:
        logger.warning(
            f"Teams in model but missing from FP data (using model-only): {', '.join(sorted(missing_from_fp))}"
        )

    if extra_in_fp:
        logger.debug(
            f"Teams in FP data but not in model (ignoring): {', '.join(sorted(extra_in_fp))}"
        )

    blended = {}

    for team, model_score in model_strength.items():
        fp_score = fp_performance.get(team, model_score)

        if team in missing_from_fp:
            logger.debug(f"  {team}: Model-only (no FP data) = {model_score:.3f}")
            blended[team] = model_score
        else:
            blended_score = blend_weight * fp_score + (1 - blend_weight) * model_score
            logger.debug(
                f"  {team}: FP={fp_score:.3f}, Model={model_score:.3f} "
                f"â†’ Blended={blended_score:.3f}"
            )
            blended[team] = blended_score

    return blended
